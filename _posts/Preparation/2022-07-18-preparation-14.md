---
title: "[NLP] í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê³¼ì •(Eng ver)"
excerpt: "[NLP] í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê³¼ì •(Eng ver)"
categories: [Preparation]
tags: [Preparation, Python]
toc: true
toc_sticky: true
---

## ðŸ’Ž í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

- í´ë Œì§•: HTML, XML íƒœê·¸ë‚˜ íŠ¹ì • ê¸°í˜¸ë¥¼ ì œê±°
- í…ìŠ¤íŠ¸ í† í°í™”(ë¬¸ìž¥ í† í°í™”, ë‹¨ì–´ í† í°í™”) -> NLTKëŠ” í† í°í™”ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ API ì œê³µ
- ìŠ¤í†± ì›Œë“œ(ë¶ˆìš©ì–´) ì œê±°
- Stemming, Lemmatization

### ë¬¸ìž¥ í† í°í™”

ë§ˆì¹¨í‘œ, \n ë“± ë¬¸ìž¥ì˜ ë§ˆì§€ë§‰ì„ ëœ»í•˜ëŠ” ê¸°í˜¸ì— ë”°ë¼ ë¶„ë¦¬í•˜ëŠ” ê²ƒ

```python
from nltk import sent_tokenize # ë¬¸ìž¥ í† í°í™” ê³¼ì •
import nltk
nltk.download('punkt') # ë§ˆì¹¨í‘œ, ê°œí–‰ ë¬¸ìž ë“±ì˜ ë°ì´í„° ì„¸íŠ¸

text_sample = 'I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s.'
sentences = sent_tokenize(text = text_sample)
print(type(sentences), len(sentences))
print(sentences)

>>>
<class 'list'> 2
['I was wondering if anyone out there could enlighten me on this car I saw the other day.', 'It was a 2-door sports car, looked to be from the late 60s/\nearly 70s.']
[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
```

### ë‹¨ì–´ í† í°í™”

ê¸°ë³¸ì ìœ¼ë¡œ ê³µë°±, ì½¤ë§ˆ, ë§ˆì¹¨í‘œ, ê°œí–‰ë¬¸ìž ë“±ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ë¶„ë¦¬

```python
from nltk import word_tokenize

sentence = 'I was wondering if anyone out there could enlighten me on this car I saw the other day.'
words = word_tokenize(sentence)
print(type(words), len(words))
print(words)

>>>
<class 'list'> 19
['I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.']
```

total

```python
from nltk import word_tokenize, sent_tokenize

# ì—¬ëŸ¬ ê°œì˜ ë¬¸ìž¥ìœ¼ë¡œ ëœ ìž…ë ¥ ë°ì´í„°ë¥¼ ë¬¸ìž¥ë³„ë¡œ ë‹¨ì–´ í† í°í™”í•˜ê²Œ ë§Œë“œëŠ” í•¨ìˆ˜
def tokenize_text(text):
  # ë¬¸ìž¥ë³„ë¡œ ë¶„ë¦¬ í† í° -> 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ì— ë¬¸ìž¥ë“¤ì´ ë“¤ì–´ìžˆë‹¤.
  sentences = sent_tokenize(text)
  print('sentences', sentences)
  # ë‹¨ì–´ ë³„ë¡œ ë¶„ë¦¬ í† í°
  word_tokens = []
  for sentence in sentences: # ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¬¸ìž¥ í•˜ë‚˜ë¥¼ ê°€ì§€ê³  ì™€ì•¼ë˜ë‹ˆê¹Œ ë°˜ë³µë¬¸ ì‚¬ìš©.
    print('sentence', sentence)
    word_tokens.append(word_tokenize(sentence))
  return word_tokens
```

### ìŠ¤í†± ì›Œë“œ ì œê±°

ì˜ì–´ì—ì„œëŠ” `is, the, a, will` ë“± ë¬¸ë§¥ì ìœ¼ë¡œ ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ë¥¼ ë§í•˜ë©° í•œêµ­ì–´ì—ì„œëŠ” `'ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ìž˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ìž','ì—','ì™€','í•œ','í•˜ë‹¤'` ê°€ ìžˆë‹¤.
nltkì—ì„œ ì˜ì–´ì— ëŒ€í•œ stop wordëŠ” ì œê³µí•œë‹¤.

```python
import nltk
nltk.download('stopwords')

print('ì˜ì–´ stop words ê°œìˆ˜:', len(nltk.corpus.stopwords.words('english')))
print(nltk.corpus.stopwords.words('english')[:20])

>>>
[29]
0ì´ˆ
import nltk
nltk.download('stopwords')

print('ì˜ì–´ stop words ê°œìˆ˜:', len(nltk.corpus.stopwords.words('english')))
print(nltk.corpus.stopwords.words('english')[:20])
ì˜ì–´ stop words ê°œìˆ˜: 179
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
```

stop word ì œê±°

```python
import nltk

stopwords = nltk.corpus.stopwords.words('english')
all_tokens = []
text_sample = 'I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s.'
word_tokens = tokenize_text(text_sample)
print('word_tokens', word_tokens)
for sentence in word_tokens: # ë‹¨ì–´ í† í°í™” ëœ í•œ ë¬¸ìž¥ í•˜ë‚˜ ë‹¹ ë°˜ë³µë¬¸ì„ ëŒë¦¬ë©´ì„œ word í•˜ë‚˜ë¡œ ì ‘ê·¼
  filtered_words = [] # stopwordë¥¼ ëº€ ë‚˜ë¨¸ì§€
  # ê°œë³„ ë¬¸ìž¥ë³„ë¡œ í† í°í™”ëœ ë¬¸ìž¥ list ì— ëŒ€í•´ ìŠ¤í†± ì›Œë“œë¥¼ ì œê±°í•˜ëŠ” ë°˜ë³µë¬¸
  for word in sentence:
    # ì†Œë¬¸ìžë¡œ ëª¨ë‘ ë³€í™˜í•©ë‹ˆë‹¤.
    word = word.lower()
    # í† í°í™”ëœ ê°œë³„ ë‹¨ì–´ê°€ ìŠ¤í†± ì›Œë“œì˜ ë‹¨ì–´ì— í¬í•¨ë˜ì§€ ì•Šìœ¼ë©´ word_tokens ì— ì¶”ê°€
    if word not in stopwords:
      filtered_words.append(word)
  all_tokens.append(filtered_words)
print('all_tokens', all_tokens)

>>>
word_tokens [['I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.'], ['It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.']]
all_tokens [['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', '.'], ['2-door', 'sports', 'car', ',', 'looked', 'late', '60s/', 'early', '70s', '.']]
```

### Stemmingê³¼ Lemmatization

ê³¼ê±°/í˜„ìž¬, 3ì¸ì¹­ ë‹¨ìˆ˜ ì—¬ë¶€, ì§„í–‰í˜• ë“± ë§Žì€ ì¡°ê±´ì— ë”°ë¼ ì›ëž˜ ë‹¨ì–´ê°€ ë³€í™”í•œë‹¤.
work
works
worked
working ...

- stemming: ì›í˜• ë‹¨ì–´ë¡œ ë³€í™˜ ì‹œ ì¼ë°˜ì ì¸ ë°©ë²•ì„ ì ìš©í•˜ê±°ë‚˜ ë” ë‹¨ìˆœí™”ëœ ë°©ë²•ì„ ì ìš© -> ì›ëž˜ ë‹¨ì–´ì—ì„œ ì¼ë¶€ ì² ìžê°€ í›¼ì†ëœ ì–´ê·¼ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ëŠ” ê²½í–¥ì´ ìžˆë‹¤.
- Lemmatization: Stemming ë³´ë‹¤ ë³€í™˜ì— ë” ì˜¤ëžœ ê¸°ê°„ì´ í•„ìš”í•˜ë‹¤.

NLTKëŠ” ë‹¤ì–‘í•œ Stemmerì„ ì œê³µ
Porter, Lancaster, Snowball Stemmer
Lemmatizationì¸ ê²½ìš°ëŠ” WordNetLemmatizerì„ ì œê³µ

- Stemming

  ```python
  from nltk.stem import LancasterStemmer
  stemmer = LancasterStemmer()

  print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))
  print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))
  print(stemmer.stem('happier'), stemmer.stem('happiest'))
  print(stemmer.stem('fancier'), stemmer.stem('fanciest'))

  >>>
  work work work
  amus amus amus
  happy happiest
  fant fanciest
  ```

- Lemmatization

  ```python
  from nltk.stem import WordNetLemmatizer
  import nltk
  nltk.download('omw-1.4')
  nltk.download('wordnet') # ì¼ë°˜ì ìœ¼ë¡œ ì›í˜• ë‹¨ì–´ ì¶”ì¶œì„ ìœ„í•´ ë‹¨ì–´ì˜ í’ˆì‚¬ë¥¼ ìž…ë ¥

  lemma = WordNetLemmatizer()
  print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))
  print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))
  print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))

  >>>
  amuse amuse amuse
  happy happy
  fancy fancy
  ```
