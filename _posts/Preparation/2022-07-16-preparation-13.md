---
title: "[NLP] ê°ì • ë¶„ì„ Sentiment Analysis"
excerpt: "[NLP] ê°ì • ë¶„ì„ Sentiment Analysis"
categories: [Preparation]
tags: [Preparation, WEB, HTML/CSS/Javascript]
toc: true
toc_sticky: true
---

ğŸ“Œ <https://www.youtube.com/watch?v=7GUoDHxN5NM&list=PL7ZVZgsnLwEEoHQAElEPg7l7T6nt25I3N&index=12>

## íŒŒì´ì¬ìœ¼ë¡œ ê°ì • ë¶„ì„í•˜ëŠ” ë°©ë²•

1. ê°ì • ì–´íœ˜ ì‚¬ì „ì„ ì´ìš©í•œ ê°ì • ìƒíƒœ ë¶„ë¥˜
   - ë¯¸ë¦¬ ë¶„ë¥˜í•´ë‘” ê°ì •ì–´ ì‚¬ì „ì„ í†µí•´ ë¶„ì„í•˜ê³ ì í•˜ëŠ” í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ë“¤ì„ ì‚¬ì „ì— ê¸°ë°˜í•´ ë¶„ë¥˜í•˜ê³ , ê·¸ ê°ì •ê°€ë¥¼ ê³„ì‚°
2. ê¸°ê³„í•™ìŠµì„ ì´ìš©í•œ ê°ì • ìƒíƒœ ë¶„ë¥˜
   - ë¶„ì„ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ í›ˆë ¨ ë°ì´í„°ë¡œ ì‚¬ìš©í•´ ê·¸ë¡œë¶€í„° í…ìŠ¤íŠ¸ì˜ ê°ì • ìƒíƒœë¥¼ ë¶„ë¥˜
   - ì´ë•Œ ì‚¬ìš©ë˜ëŠ” í›ˆë ¨ ë°ì´í„°ëŠ” ì‚¬ìš©ìê°€ ë¶„ë¥˜í•œ ê°ì • ë¼ë²¨ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.
   - ì´ë¥¼ ì¸ê³µ ì‹ ê²½ë§, ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ ë“±ì˜ ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜

## ì§„í–‰ ê³¼ì •

### 1ï¸âƒ£ ê°ì • ì‚¬ì „ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

afinn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í–ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì˜ì–´ì— ëŒ€í•œ ê¸ì •, ë¶€ì •ì— ê´€í•œ ê°ì • ì‚¬ì „ì„ ì œê³µí•œë‹¤.

```python
!pip install afinn

from sklearn.datasets import fetch_20newsgroups

newsdata = fetch_20newsgroups(subset='train')
newsdata.data[0]

>>>
From: lerxst@wam.umd.edu (where's my thing)
Subject: WHAT car is this!?
Nntp-Posting-Host: rac3.wam.umd.edu
Organization: University of Maryland, College Park
Lines: 15

 I was wondering if anyone out there could enlighten me on this car I saw
the other day. It was a 2-door sports car, looked to be from the late 60s/
early 70s. It was called a Bricklin. The doors were really small. In addition,
the front bumper was separate from the rest of the body. This is
all I know. If anyone can tellme a model name, engine specs, years
of production, where this car is made, history, or whatever info you
have on this funky looking car, please e-mail.

Thanks,
- IL
   ---- brought to you by your neighborhood Lerxst ----
```

ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ê³ , ì²« ë²ˆì§¸ ë°ì´í„°ë¥¼ ì¶œë ¥í•œ í…ìŠ¤íŠ¸ì´ë‹¤.

- sklearn.datasets ?
- â“fetch_20newsgroupsëŠ” ë­˜ê¹Œ?
- â“subset='train'ì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ” ê±¸ê¹Œ?

### 2ï¸âƒ£ ê°ì • ìƒíƒœ ë¶„ë¥˜ ë° ì‹œê°í™”

```python
from afinn import Afinn
import numpy as np
import matplotlib.pyplot as plt

afinn = Afinn()

plt.style.use('seaborn-white')

positive = 0
neutral = 0
negative = 0

for i in newsdata.data:
  score = afinn.score(i)
  if score > 0:
    positive += 1
  elif score == 0:
    neutral += 1
  else:
    negative += 1

print('postive', positive)
print('neutral', neutral)
print('negative', negative)

plt.bar(np.arange(3), [positive, neutral, negative])
plt.xticks(np.arange(3), ['positive', 'neutral', 'negative'])
plt.show()

>>>
postive 6923
neutral 809
negative 3582
```

![download1](https://user-images.githubusercontent.com/96654391/179365831-9daaa603-1fb2-4373-b568-28b2af879f44.png)

- _afinn.score(newsdata.data[i])_ ë¥¼ í†µí•´ì„œ ê°ì • ì ìˆ˜ë¥¼ ê°ê° ì¶œë ¥í•  ìˆ˜ ìˆë‹¤.

```python
plt.bar(np.arange(3), [positive, neutral, negative])
plt.xticks(np.arange(3), ['positive', 'neutral', 'negative'])
```

ë¥¼ í†µí•´ì„œ ë§‰ëŒ€ ê·¸ë˜í”„ì˜ ë§‰ëŒ€ ë¶€ë¶„ì„ ê°œìˆ˜ë¡œ ì •ì˜í–ˆê³  xticksë¥¼ í†µí•´ì„œ x ë¼ë²¨ì´ ë“¤ì–´ê°ˆ ë‹¨ì–´ë¥¼ ì •ì˜í–ˆë‹¤.

## ê¸°ê³„í•™ìŠµì„ ì´ìš©í•œ ê°ì • ë¶„ì„

ì—¬ê¸°ì„œë¶€í„° ë‹¤ì‹œ ì„í¬íŠ¸ë¶€í„° ì‹œì‘!

- í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ konlpyì™€ í˜•íƒœì†Œ ë¶„ì„ê¸° MeCab ì„¤ì¹˜
  ```python
  !set -x \
  && pip install konlpy \
  && curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x
  ```
- default

  ```python
  import re
  import urllib.request
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  plt.style.use('seaborn-white')

  from konlpy.tag import Mecab
  from tensorflow.keras.preprocessing.text import Tokenizer
  from tensorflow.keras.preprocessing.sequence import pad_sequences
  ```

### 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ: `urllib.request` ì‚¬ìš©

```python
train_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')
test_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')

train_data = pd.read_table(train_file)
test_data = pd.read_table(test_file)
```

### 2ï¸âƒ£ ì¤‘ë³µ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬

- ë°ì´í„° ì¤‘ë³µ ì œê±°

  - ì¤‘ë³µ ë° null ê°’ ì œê±°

    ```python
    print(train_data['document'].nunique())
    print(train_data['label'].nunique())

    train_data.drop_duplicates(subset=['document'], inplace=True) # ì¤‘ë³µ ì œê±°

    print(train_data.isnull().sum())
    train_data = train_data.dropna(how='any')

    >>>
    146182
    2

    id          0
    document    1
    label       0
    dtype: int64
    ```

### 3ï¸âƒ£ ë°ì´í„° ì •ì œ

ì´ ê³¼ì •ì„ í†µí•´ì„œ í•œê¸€(?) ë§Œ ë‚¨ê²¨ë†“ëŠ”ë‹¤. ì½¤ë§ˆ, ì˜¨ì  ë“±ì€ ì œì™¸ì‹œí‚´.

```python
train_data['document'] = train_data['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]", "")
train_data[:10]
train_data = train_data.dropna(how='any')

test_data.drop_duplicates(subset=['document'], inplace=True)
test_data['document'] = test_data['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]", "")
test_data = test_data.dropna(how='any')
```

### 4ï¸âƒ£ í† í°í™”

`'ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤'` ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ë¶„ë¦¬í•˜ê³ , ë¶ˆìš©ì–´ëŠ” ì œê±°í•œë‹¤.

```python
stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']

mecab = Mecab()

X_train = []
for sentence in train_data['document']:
  X_train.append([word for word in mecab.morphs(sentence) if not word in stopwords])

print(X_train[:10])

>>>
[['ì•„', 'ë”', 'ë¹™', 'ì§„ì§œ', 'ì§œì¦', 'ë‚˜', 'ë„¤ìš”', 'ëª©ì†Œë¦¬'], ['í ', 'í¬ìŠ¤í„°', 'ë³´ê³ ', 'ì´ˆë”©', 'ì˜í™”', 'ì¤„', 'ì˜¤ë²„', 'ì—°ê¸°', 'ì¡°ì°¨', 'ê°€ë³', 'ì§€', 'ì•Š', 'êµ¬ë‚˜'], ['ë„ˆë¬´', 'ì¬', 'ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤'], ['êµë„ì†Œ', 'ì´ì•¼ê¸°', 'êµ¬ë¨¼', 'ì†”ì§íˆ', 'ì¬ë¯¸', 'ì—†', 'ë‹¤', 'í‰ì ', 'ì¡°ì •'], ['ì‚¬ì´ëª¬í˜ê·¸', 'ìµì‚´', 'ìŠ¤ëŸ°', 'ì—°ê¸°', 'ë‹ë³´ì˜€', 'ë˜', 'ì˜í™”', 'ìŠ¤íŒŒì´ë”ë§¨', 'ì—ì„œ', 'ëŠ™', 'ì–´', 'ë³´ì´', 'ê¸°', 'ë§Œ', 'í–ˆ', 'ë˜', 'ì»¤ìŠ¤í‹´', 'ë˜ìŠ¤íŠ¸', 'ë„ˆë¬´ë‚˜', 'ì´ë»', 'ë³´ì˜€', 'ë‹¤'], ['ë§‰', 'ê±¸ìŒë§ˆ', 'ë—€', 'ì„¸', 'ë¶€í„°', 'ì´ˆë“±', 'í•™êµ', 'í•™ë…„', 'ìƒ', 'ì¸', 'ì‚´ìš©', 'ì˜í™”', 'ã…‹ã…‹ã…‹', 'ë³„ë°˜', 'ê°œ', 'ì•„ê¹Œì›€'], ['ì›ì‘', 'ê¸´ì¥ê°', 'ì„', 'ì œëŒ€ë¡œ', 'ì‚´ë ¤', 'ë‚´', 'ì§€', 'ëª»í–ˆ', 'ë‹¤'], ['ë³„', 'ë°˜ê°œ', 'ì•„ê¹', 'ë‹¤', 'ìš•', 'ë‚˜ì˜¨ë‹¤', 'ì´ì‘ê²½', 'ê¸¸ìš©ìš°', 'ì—°ê¸°', 'ìƒí™œ', 'ëª‡', 'ë…„', 'ì¸ì§€', 'ì •ë§', 'ë°œ', 'ë¡œ', 'í•´ë„', 'ê·¸ê²ƒ', 'ë³´ë‹¨', 'ë‚«', 'ê²Ÿ', 'ë‹¤', 'ë‚©ì¹˜', 'ê°ê¸ˆ', 'ë§Œ', 'ë°˜ë³µ', 'ë°˜ë³µ', 'ë“œë¼ë§ˆ', 'ê°€ì¡±', 'ì—†', 'ë‹¤', 'ì—°ê¸°', 'ëª»', 'í•˜', 'ì‚¬ëŒ', 'ë§Œ', 'ëª¨ì—¿', 'ë„¤'], ['ì•¡ì…˜', 'ì—†', 'ëŠ”ë°', 'ì¬ë¯¸', 'ìˆ', 'ëª‡', 'ì•ˆ', 'ë˜', 'ì˜í™”'], ['ì™œ', 'ì¼€', 'í‰ì ', 'ë‚®', 'ê±´ë°', 'ê½¤', 'ë³¼', 'ë§Œ', 'í•œë°', 'í—ë¦¬ìš°ë“œ', 'ì‹', 'í™”ë ¤', 'í•¨', 'ë§Œ', 'ë„ˆë¬´', 'ê¸¸ë“¤ì—¬ì ¸', 'ìˆ', 'ë‚˜']]
```

X_testë„ ë§ˆì°¬ê°€ì§€ë¡œ ìˆ˜í–‰!

```python
X_test = []
for sentence in test_data['document']:
  X_test.append([word for word in mecab.morphs(sentence) if not word in stopwords])
```

### 5ï¸âƒ£ ë¹ˆë„ìˆ˜ ë‚®ì€ ë‹¨ì–´ ì œê±°

```python
threshold = 3
words_cnt = len(tokenizer.word_index)
rare_cnt = 0
words_freq = 0
rare_freq = 0

for key, value in tokenizer.word_counts.items():
words_freq = words_freq + value

if value < threshold:
rare_cnt += 1
rare_freq = rare_freq + value

print("ì „ì²´ ë‹¨ì–´ ìˆ˜ : ", words_cnt)
print("ë¹ˆë„ê°€ {} ì´í•˜ì¸ íšŒê·€ ë‹¨ì–´ ìˆ˜ : {}".format(threshold - 1, rare_cnt))
print("íšŒê·€ ë‹¨ì–´ ë¹„ìœ¨: {}".format((rare_cnt / words_cnt) * 100))
print("íšŒê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨: {}".format((rare_freq / words_freq) * 100))

>>>
ì „ì²´ ë‹¨ì–´ ìˆ˜ :  49946
ë¹ˆë„ê°€ 2 ì´í•˜ì¸ íšŒê·€ ë‹¨ì–´ ìˆ˜ : 28320
íšŒê·€ ë‹¨ì–´ ë¹„ìœ¨: 56.70123733632323
íšŒê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨: 1.7606762208782198
```

### 6ï¸âƒ£ íŒ¨ë”©

- ë¦¬ë·°ì˜ ì „ë°˜ì ì¸ ê¸¸ì´ë¥¼ í™•ì¸
- ëª¨ë¸ì˜ ì…ë ¥ì„ ìœ„í•´ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶°ì¤Œ

```python
print('ë¦¬ë·° ìµœëŒ€ ê¸¸ì´:', max(len(l) for l in X_train))
print('ë¦¬ë·° í‰ê·  ê¸¸ì´:', sum(map(len, X_train))/len(X_train))

>>>
ë¦¬ë·° ìµœëŒ€ ê¸¸ì´: 83
ë¦¬ë·° í‰ê·  ê¸¸ì´: 13.801382583574082

plt.hist([len(s) for s in X_train], bins=50)
plt.xlabel('length of Samples')
plt.ylabel('Number of Samples')
plt.show()
```

![download2](https://user-images.githubusercontent.com/96654391/179431295-53b753b6-300b-4bfb-9d09-0b789a5b5c71.png)

ìµœëŒ€ ê¸¸ì´ë¥¼ 30 ë˜ëŠ” 60ìœ¼ë¡œ ì¡ìŒ -> ë‘˜ ì¤‘ ìƒê´€ ã„´

```python
max_len = 60
X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)
```

### 7ï¸âƒ£ ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ

```python
from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential

model = Sequential()
model.add(Embedding(vocab_size, 100)) # output = 100
model.add(LSTM(128, dropout=0.3))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
             metrics=['acc'])
model.summary()

```
