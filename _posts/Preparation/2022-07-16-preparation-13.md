---
title: "[NLP] ê°ì • ë¶„ì„ Sentiment Analysis"
excerpt: "[NLP] ê°ì • ë¶„ì„ Sentiment Analysis"
categories: [Preparation]
tags: [Preparation, WEB, HTML/CSS/Javascript]
toc: true
toc_sticky: true
---

ğŸ“Œ <https://www.youtube.com/watch?v=7GUoDHxN5NM&list=PL7ZVZgsnLwEEoHQAElEPg7l7T6nt25I3N&index=12>

## íŒŒì´ì¬ìœ¼ë¡œ ê°ì • ë¶„ì„í•˜ëŠ” ë°©ë²•

1. ê°ì • ì–´íœ˜ ì‚¬ì „ì„ ì´ìš©í•œ ê°ì • ìƒíƒœ ë¶„ë¥˜
   - ë¯¸ë¦¬ ë¶„ë¥˜í•´ë‘” ê°ì •ì–´ ì‚¬ì „ì„ í†µí•´ ë¶„ì„í•˜ê³ ì í•˜ëŠ” í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ë“¤ì„ ì‚¬ì „ì— ê¸°ë°˜í•´ ë¶„ë¥˜í•˜ê³ , ê·¸ ê°ì •ê°€ë¥¼ ê³„ì‚°
2. ê¸°ê³„í•™ìŠµì„ ì´ìš©í•œ ê°ì • ìƒíƒœ ë¶„ë¥˜
   - ë¶„ì„ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ í›ˆë ¨ ë°ì´í„°ë¡œ ì‚¬ìš©í•´ ê·¸ë¡œë¶€í„° í…ìŠ¤íŠ¸ì˜ ê°ì • ìƒíƒœë¥¼ ë¶„ë¥˜
   - ì´ë•Œ ì‚¬ìš©ë˜ëŠ” í›ˆë ¨ ë°ì´í„°ëŠ” ì‚¬ìš©ìê°€ ë¶„ë¥˜í•œ ê°ì • ë¼ë²¨ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.
   - ì´ë¥¼ ì¸ê³µ ì‹ ê²½ë§, ì˜ì‚¬ ê²°ì • íŠ¸ë¦¬ ë“±ì˜ ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜

## ê°ì • ì–´íœ˜ ì‚¬ì „ì„ ì´ìš©í•œ ê°ì • ìƒíƒœ ë¶„ë¥˜ => Eng ver.

### 1ï¸âƒ£ ê°ì • ì‚¬ì „ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

afinn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í–ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì˜ì–´ì— ëŒ€í•œ ê¸ì •, ë¶€ì •ì— ê´€í•œ ê°ì • ì‚¬ì „ì„ ì œê³µí•œë‹¤.

```python
!pip install afinn

from sklearn.datasets import fetch_20newsgroups

newsdata = fetch_20newsgroups(subset='train')
newsdata.data[0]

>>>
From: lerxst@wam.umd.edu (where's my thing)
Subject: WHAT car is this!?
Nntp-Posting-Host: rac3.wam.umd.edu
Organization: University of Maryland, College Park
Lines: 15

 I was wondering if anyone out there could enlighten me on this car I saw
the other day. It was a 2-door sports car, looked to be from the late 60s/
early 70s. It was called a Bricklin. The doors were really small. In addition,
the front bumper was separate from the rest of the body. This is
all I know. If anyone can tellme a model name, engine specs, years
of production, where this car is made, history, or whatever info you
have on this funky looking car, please e-mail.

Thanks,
- IL
   ---- brought to you by your neighborhood Lerxst ----
```

ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ê³ , ì²« ë²ˆì§¸ ë°ì´í„°ë¥¼ ì¶œë ¥í•œ í…ìŠ¤íŠ¸ì´ë‹¤.

- sklearn.datasets ? : ë°ì´í„°ì…‹
- â“fetch_20newsgroupsëŠ” ë­˜ê¹Œ? : ì‚¬ì´í‚·ëŸ°ì—ì„œ ì œê³µí•˜ëŠ” ë°ì´í„°ì…‹ ì¤‘ í•˜ë‚˜
- â“subset='train'ì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ” ê±¸ê¹Œ?

### 2ï¸âƒ£ ê°ì • ìƒíƒœ ë¶„ë¥˜ ë° ì‹œê°í™”

```python
from afinn import Afinn
import numpy as np
import matplotlib.pyplot as plt

afinn = Afinn()

plt.style.use('seaborn-white')

positive = 0
neutral = 0
negative = 0

for i in newsdata.data:
  score = afinn.score(i)
  if score > 0:
    positive += 1
  elif score == 0:
    neutral += 1
  else:
    negative += 1

print('postive', positive)
print('neutral', neutral)
print('negative', negative)

plt.bar(np.arange(3), [positive, neutral, negative])
plt.xticks(np.arange(3), ['positive', 'neutral', 'negative'])
plt.show()

>>>
postive 6923
neutral 809
negative 3582
```

![download1](https://user-images.githubusercontent.com/96654391/179365831-9daaa603-1fb2-4373-b568-28b2af879f44.png)

- _afinn.score(newsdata.data[i])_ ë¥¼ í†µí•´ì„œ ê°ì • ì ìˆ˜ë¥¼ ê°ê° ì¶œë ¥í•  ìˆ˜ ìˆë‹¤.

```python
plt.bar(np.arange(3), [positive, neutral, negative])
plt.xticks(np.arange(3), ['positive', 'neutral', 'negative'])
```

ë¥¼ í†µí•´ì„œ ë§‰ëŒ€ ê·¸ë˜í”„ì˜ ë§‰ëŒ€ ë¶€ë¶„ì„ ê°œìˆ˜ë¡œ ì •ì˜í–ˆê³  xticksë¥¼ í†µí•´ì„œ x ë¼ë²¨ì´ ë“¤ì–´ê°ˆ ë‹¨ì–´ë¥¼ ì •ì˜í–ˆë‹¤.

## ê¸°ê³„í•™ìŠµì„ ì´ìš©í•œ ê°ì • ë¶„ì„ => Kor ver.

ì—¬ê¸°ì„œë¶€í„° ë‹¤ì‹œ ì„í¬íŠ¸ë¶€í„° ì‹œì‘!

- í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ konlpyì™€ í˜•íƒœì†Œ ë¶„ì„ê¸° MeCab ì„¤ì¹˜
  ```python
  !set -x \
  && pip install konlpy \
  && curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x
  ```
- default

  ```python
  import re
  import urllib.request
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  plt.style.use('seaborn-white')

  from konlpy.tag import Mecab
  from tensorflow.keras.preprocessing.text import Tokenizer
  from tensorflow.keras.preprocessing.sequence import pad_sequences
  ```

### 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ: `urllib.request` ì‚¬ìš©

```python
train_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')
test_file = urllib.request.urlopen('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')

train_data = pd.read_table(train_file)
test_data = pd.read_table(test_file)
```

### 2ï¸âƒ£ ì¤‘ë³µ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬

- ë°ì´í„° ì¤‘ë³µ ë° null ì œê±°

  ```python
  print(train_data['document'].nunique())
  print(train_data['label'].nunique())

  train_data.drop_duplicates(subset=['document'], inplace=True) # ì¤‘ë³µ ì œê±°

  print(train_data.isnull().sum())
  train_data = train_data.dropna(how='any')

  >>>
  146182
  2

  id          0
  document    1
  label       0
  dtype: int64
  ```

  - unique(): ë°ì´í„°ì— ê³ ìœ ê°’ë“¤ì´ ë­ê°€ ìˆëŠ”ì§€ ì•Œê³  ì‹¶ì„ ë•Œ
  - nunique(): ë°ì´í„°ì— ê³ ìœ ê°’ë“¤ì˜ ìˆ˜ë¥¼ ì¶œë ¥í•´ì£¼ëŠ” í•¨ìˆ˜
  - value_counts(): ê°’ë³„ë¡œ ë°ì´í„°ì˜ ìˆ˜ë¥¼ ì¶œë ¥í•´ì£¼ëŠ” í•¨ìˆ˜
  - drop_duplicates(): ì¤‘ë³µëœ ì—´ì˜ ê°’ì„ ì œê±°
  - dropna(): any -> 1ê°œë¼ë„ Nanì´ ì¡´ì¬ì‹œ drop, all -> ëª¨ë‘ NaNê°’ì´ ì¡´ì¬ì‹œ drop

### 3ï¸âƒ£ ë°ì´í„° ì •ì œ

ì´ ê³¼ì •ì„ í†µí•´ì„œ í•œê¸€(?) ë§Œ ë‚¨ê²¨ë†“ëŠ”ë‹¤. ì½¤ë§ˆ, ì˜¨ì  ë“±ì€ ì œì™¸ì‹œí‚´.

```python
train_data['document'] = train_data['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]", "")
train_data[:10]
train_data = train_data.dropna(how='any')

test_data.drop_duplicates(subset=['document'], inplace=True)
test_data['document'] = test_data['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]", "")
test_data = test_data.dropna(how='any')
```

### 4ï¸âƒ£ í† í°í™”

`'ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤'` ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ë¶„ë¦¬í•˜ê³ , ë¶ˆìš©ì–´ëŠ” ì œê±°í•œë‹¤.

```python
stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']

mecab = Mecab()

X_train = []
for sentence in train_data['document']:
  X_train.append([word for word in mecab.morphs(sentence) if not word in stopwords])

print(X_train[:10])

>>>
[['ì•„', 'ë”', 'ë¹™', 'ì§„ì§œ', 'ì§œì¦', 'ë‚˜', 'ë„¤ìš”', 'ëª©ì†Œë¦¬'], ['í ', 'í¬ìŠ¤í„°', 'ë³´ê³ ', 'ì´ˆë”©', 'ì˜í™”', 'ì¤„', 'ì˜¤ë²„', 'ì—°ê¸°', 'ì¡°ì°¨', 'ê°€ë³', 'ì§€', 'ì•Š', 'êµ¬ë‚˜'], ['ë„ˆë¬´', 'ì¬', 'ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤'], ['êµë„ì†Œ', 'ì´ì•¼ê¸°', 'êµ¬ë¨¼', 'ì†”ì§íˆ', 'ì¬ë¯¸', 'ì—†', 'ë‹¤', 'í‰ì ', 'ì¡°ì •'], ['ì‚¬ì´ëª¬í˜ê·¸', 'ìµì‚´', 'ìŠ¤ëŸ°', 'ì—°ê¸°', 'ë‹ë³´ì˜€', 'ë˜', 'ì˜í™”', 'ìŠ¤íŒŒì´ë”ë§¨', 'ì—ì„œ', 'ëŠ™', 'ì–´', 'ë³´ì´', 'ê¸°', 'ë§Œ', 'í–ˆ', 'ë˜', 'ì»¤ìŠ¤í‹´', 'ë˜ìŠ¤íŠ¸', 'ë„ˆë¬´ë‚˜', 'ì´ë»', 'ë³´ì˜€', 'ë‹¤'], ['ë§‰', 'ê±¸ìŒë§ˆ', 'ë—€', 'ì„¸', 'ë¶€í„°', 'ì´ˆë“±', 'í•™êµ', 'í•™ë…„', 'ìƒ', 'ì¸', 'ì‚´ìš©', 'ì˜í™”', 'ã…‹ã…‹ã…‹', 'ë³„ë°˜', 'ê°œ', 'ì•„ê¹Œì›€'], ['ì›ì‘', 'ê¸´ì¥ê°', 'ì„', 'ì œëŒ€ë¡œ', 'ì‚´ë ¤', 'ë‚´', 'ì§€', 'ëª»í–ˆ', 'ë‹¤'], ['ë³„', 'ë°˜ê°œ', 'ì•„ê¹', 'ë‹¤', 'ìš•', 'ë‚˜ì˜¨ë‹¤', 'ì´ì‘ê²½', 'ê¸¸ìš©ìš°', 'ì—°ê¸°', 'ìƒí™œ', 'ëª‡', 'ë…„', 'ì¸ì§€', 'ì •ë§', 'ë°œ', 'ë¡œ', 'í•´ë„', 'ê·¸ê²ƒ', 'ë³´ë‹¨', 'ë‚«', 'ê²Ÿ', 'ë‹¤', 'ë‚©ì¹˜', 'ê°ê¸ˆ', 'ë§Œ', 'ë°˜ë³µ', 'ë°˜ë³µ', 'ë“œë¼ë§ˆ', 'ê°€ì¡±', 'ì—†', 'ë‹¤', 'ì—°ê¸°', 'ëª»', 'í•˜', 'ì‚¬ëŒ', 'ë§Œ', 'ëª¨ì—¿', 'ë„¤'], ['ì•¡ì…˜', 'ì—†', 'ëŠ”ë°', 'ì¬ë¯¸', 'ìˆ', 'ëª‡', 'ì•ˆ', 'ë˜', 'ì˜í™”'], ['ì™œ', 'ì¼€', 'í‰ì ', 'ë‚®', 'ê±´ë°', 'ê½¤', 'ë³¼', 'ë§Œ', 'í•œë°', 'í—ë¦¬ìš°ë“œ', 'ì‹', 'í™”ë ¤', 'í•¨', 'ë§Œ', 'ë„ˆë¬´', 'ê¸¸ë“¤ì—¬ì ¸', 'ìˆ', 'ë‚˜']]
```

#### `from konlpy.tag import *`

-> KoNLPy í•œêµ­ì–´ ì²˜ë¦¬ íŒ¨í‚¤ì§€: ëŒ€í•œë¯¼êµ­ í—Œë²• ë§ë­‰ì¹˜ì¸ kolawì™€ êµ­íšŒë²•ì•ˆ ë§ë­‰ì¹˜ì¸ kobillì„ ì œê³µí•œë‹¤.

**í˜•íƒœì†Œ ë¶„ì„**

- Hannanum
- Kkma
- Komoran
- Mecab: ë©”ì¹´ë¸Œ
- Open Korean Text

**í´ë˜ìŠ¤ ë‚´ ê³µí†µ ë©”ì„œë“œ**

- nouns: ëª…ì‚¬ ì¶”ì¶œ
- morphs: ëª…ì‚¬ + í˜•íƒœì†Œ ì¶”ì¶œ
- pos: í’ˆì‚¬ ë¶€ì°©

X_testë„ ë§ˆì°¬ê°€ì§€ë¡œ ìˆ˜í–‰!

```python
X_test = []
for sentence in test_data['document']:
  X_test.append([word for word in mecab.morphs(sentence) if not word in stopwords])
```

### 5ï¸âƒ£ ë¹ˆë„ìˆ˜ ë‚®ì€ ë‹¨ì–´ ì œê±°

```python
threshold = 3
words_cnt = len(tokenizer.word_index)
rare_cnt = 0
words_freq = 0
rare_freq = 0

for key, value in tokenizer.word_counts.items():
  words_freq = words_freq + value

  if value < threshold:
    rare_cnt += 1
    rare_freq = rare_freq + value

print("ì „ì²´ ë‹¨ì–´ ìˆ˜ : ", words_cnt)
print("ë¹ˆë„ê°€ {} ì´í•˜ì¸ íšŒê·€ ë‹¨ì–´ ìˆ˜ : {}".format(threshold - 1, rare_cnt))
print("íšŒê·€ ë‹¨ì–´ ë¹„ìœ¨: {}".format((rare_cnt / words_cnt) * 100))
print("íšŒê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨: {}".format((rare_freq / words_freq) * 100))

>>>
ì „ì²´ ë‹¨ì–´ ìˆ˜ :  49946
ë¹ˆë„ê°€ 2 ì´í•˜ì¸ íšŒê·€ ë‹¨ì–´ ìˆ˜ : 28320
íšŒê·€ ë‹¨ì–´ ë¹„ìœ¨: 56.70123733632323
íšŒê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨: 1.7606762208782198
```

- word_index: ê° ë‹¨ì–´ì— ì¸ë±ìŠ¤ê°€ ì–´ë–»ê²Œ ë¶€ì—¬ë˜ì—ˆëŠ”ì§€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë¦¬í„´(ì¸ë±ìŠ¤ ê¸°ì¤€: ë¹ˆë„ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì‘ì€ ì¸ë±ìŠ¤ ë¶€ì—¬)
- word_counts: ê° ë‹¨ì–´ê°€ ëª‡ ê°œì˜€ëŠ”ì§€ ì¹´ìš´íŠ¸ëœ ê²°ê³¼ë¥¼ ë¦¬í„´(key: ë‹¨ì–´, value: ê°œìˆ˜)

### 6ï¸âƒ£ íŒ¨ë”©

- ë¦¬ë·°ì˜ ì „ë°˜ì ì¸ ê¸¸ì´ë¥¼ í™•ì¸
- ëª¨ë¸ì˜ ì…ë ¥ì„ ìœ„í•´ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶°ì¤Œ

```python
print('ë¦¬ë·° ìµœëŒ€ ê¸¸ì´:', max(len(l) for l in X_train))
print('ë¦¬ë·° í‰ê·  ê¸¸ì´:', sum(map(len, X_train))/len(X_train))

>>>
ë¦¬ë·° ìµœëŒ€ ê¸¸ì´: 83
ë¦¬ë·° í‰ê·  ê¸¸ì´: 13.801382583574082

plt.hist([len(s) for s in X_train], bins=50)
plt.xlabel('length of Samples')
plt.ylabel('Number of Samples')
plt.show()
```

![download2](https://user-images.githubusercontent.com/96654391/179431295-53b753b6-300b-4bfb-9d09-0b789a5b5c71.png)

ìµœëŒ€ ê¸¸ì´ë¥¼ 30 ë˜ëŠ” 60ìœ¼ë¡œ ì¡ìŒ -> ë‘˜ ì¤‘ ìƒê´€ ã„´

```python
max_len = 60
X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)
```

- pad_sequences: ì¼€ë¼ìŠ¤ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë©”ì†Œë“œ

### 7ï¸âƒ£ ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ

```python
from tensorflow.keras.layers import Embedding, Dense, LSTM
from tensorflow.keras.models import Sequential

model = Sequential()
model.add(Embedding(vocab_size, 100)) # output = 100
model.add(LSTM(128, dropout=0.3))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
             metrics=['acc'])
model.summary()
```

í•™ìŠµ

```python
from tensorflow import keras

checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5', save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=10, batch_size=60, validation_split=0.2, callbacks=[checkpoint_cb, early_stopping_cb])

>>>
Epoch 1/10
1939/1939 [==============================] - 344s 175ms/step - loss: 0.3980 - acc: 0.8203 - val_loss: 0.3533 - val_acc: 0.8424
Epoch 2/10
1939/1939 [==============================] - 333s 172ms/step - loss: 0.3319 - acc: 0.8563 - val_loss: 0.3310 - val_acc: 0.8557
Epoch 3/10
1939/1939 [==============================] - 329s 170ms/step - loss: 0.3067 - acc: 0.8697 - val_loss: 0.3250 - val_acc: 0.8612
Epoch 4/10
1939/1939 [==============================] - 330s 170ms/step - loss: 0.2889 - acc: 0.8788 - val_loss: 0.3157 - val_acc: 0.8641
Epoch 5/10
1939/1939 [==============================] - 327s 169ms/step - loss: 0.2745 - acc: 0.8871 - val_loss: 0.3138 - val_acc: 0.8644
Epoch 6/10
1939/1939 [==============================] - 327s 168ms/step - loss: 0.2628 - acc: 0.8926 - val_loss: 0.3229 - val_acc: 0.8638
Epoch 7/10
1939/1939 [==============================] - 327s 169ms/step - loss: 0.2519 - acc: 0.8980 - val_loss: 0.3162 - val_acc: 0.8688
Epoch 8/10
1939/1939 [==============================] - 330s 170ms/step - loss: 0.2413 - acc: 0.9024 - val_loss: 0.3161 - val_acc: 0.8671
```

í‰ê°€

```python
model.evaluate(X_test, y_test)

>>>
1537/1537 [==============================] - 38s 25ms/step - loss: 0.3218 - acc: 0.8609
[0.32183122634887695, 0.860874354839325]
```

### 8ï¸âƒ£ ì‹œê°í™”

```python
hist_dict = history.history
loss = hist_dict['loss']
val_loss = hist_dict['val_loss']
acc = hist_dict['acc']
val_acc = hist_dict['val_acc']

plt.plot(loss, 'b--', label='training loss')
plt.plot(val_loss, 'r:', label='validation loss')
plt.legend()
plt.grid()

plt.figure()
plt.plot(acc, 'b--', label='training accuracy')
plt.plot(val_acc, 'r:', label='validation accuracy')
plt.legend()
plt.grid()
```

![download1](https://user-images.githubusercontent.com/96654391/179434342-1cbd4455-7779-4e74-937c-636f7ad3a80f.png)

![download2](https://user-images.githubusercontent.com/96654391/179434346-648dc13d-b11a-4f18-8386-d153a650b4cb.png)

### ğŸŒ— ê°ì • ì˜ˆì¸¡

```python
def sentiment_predict(new_sentence):
  new_token = [word for word in mecab.morphs(new_sentence) if not word in stopwords]
  new_sequences = tokenizer.texts_to_sequences([new_token])
  new_pad = pad_sequences(new_sequences, maxlen = max_len)
  score = float(model.predict(new_pad))

  if score > 0.5:
    print("{} -> ê¸ì • ({:.2f}%)".format(new_sentence, score * 100))
  else:
    print("{} -> ë¶€ì • ({:.2f}%)".format(new_sentence, (1-score) * 100))

sentiment_predict("ì •ë§ ì¬ë¯¸ìˆê³  í¥ë¯¸ì§„ì§„ í–ˆì–´ìš”.")
sentiment_predict("ì–´ë–»ê²Œ ì´ë ‡ê²Œ ì§€ë£¨í•˜ê³  ì¬ë¯¸ì—†ì£ ?")
sentiment_predict("ë°°ìš° ì—°ê¸°ë ¥ì´ ëŒ€ë°•ì…ë‹ˆë‹¤. ")
sentiment_predict("ë¶„ìœ„ê¸°ê°€ ì–´ë‘ì›Œìš”")
sentiment_predict("ìŠ¤í† ë¦¬ê°€ ë³µì¡í•´ìš” ")

>>>
ì •ë§ ì¬ë¯¸ìˆê³  í¥ë¯¸ì§„ì§„ í–ˆì–´ìš”. -> ê¸ì • (98.87%)
ì–´ë–»ê²Œ ì´ë ‡ê²Œ ì§€ë£¨í•˜ê³  ì¬ë¯¸ì—†ì£ ? -> ë¶€ì • (99.74%)
ë°°ìš° ì—°ê¸°ë ¥ì´ ëŒ€ë°•ì…ë‹ˆë‹¤.  -> ê¸ì • (96.64%)
ë¶„ìœ„ê¸°ê°€ ì–´ë‘ì›Œìš” -> ë¶€ì • (81.39%)
ìŠ¤í† ë¦¬ê°€ ë³µì¡í•´ìš”  -> ê¸ì • (85.32%)
```

- texts_to_sequences(): ê° ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
  ex.
  ```python
  {'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}
  [[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]
  ```
