---
title: "[week 6] Object Detection: YOLOv1~v2"
excerpt: "[week 6] Object Detection: YOLOv1~v2"
categories: [GDSC ML]
tags: [GDSC ML, Python]
toc: true
toc_sticky: true
---

## Introduction

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/200993738-d0dcb349-6b9b-4a62-a869-59e61b0ab4d4.png">

### One-Stage Detector

이미지가 들어오면 Conv & FC Layers를 거져서 output을 만들게 되고 이를 통해서 Classification과 Box Regression을 진행한다.

### Two-Stage Detector

원본 이미지 -> Region Proposal -> 객체가 있을 것 같은 영역을 찾아낸다. <br>
원본 이밎 -> Classification를 통해서 Feature map을 뽑아낸 후 -> Proposed Regions를 Feature map에 투영시켜서 Classification과 Box Regression을 진행

## YOLO

**_Main Contribution_** <br>

1. object detection 을 regression problem으로 관점 전환
2. Unified Architecture: 하나의 신경망으로 classification & localization 예측
3. DPM, R-CNN 모델보다 속도 개선
4. 여러 도메인에서 object detection 가능

### Unified Detection

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/201232150-2235cd4d-7932-4ff1-89bd-4ae8742a0e02.png"> <br>

**_one-stage detection으로 통합_** <br>

1. input image를 S x S 그리드로 나눈다.
2. Box Regression + Class Probability을 구함 <br>

<img width="1000" alt="download2" src="https://user-images.githubusercontent.com/96654391/201233063-3dbe496c-7e22-41f7-a0d3-19586ee3c7aa.png"> <br>
하나의 그리드 셀에서 예측할 BBox를 2개, 클래스의 종류를 20이라고 할 때 <br>

Box Regression 진행 과정 <br>
하나의 그리드 셀을 기준으로 예측한 첫 번째 BBox는 진한 파란색 박스이고, 여기서 나온 output으로는 BBox의 정 중앙 좌표일 x, y 그리고 input의 w, h를 셀만큼 나누어서 정규화한 w, h(0~1) 그리고 물체가 BBox 내에 있는지 없는지 나타내는 Pc이다. <br>

Classification <br>
각 클래스에 대해서 해당 객체가 어떤 클래스에 해당할지에 대한 확률을 나타낸다. <br>
<img width="1000" alt="download3" src="https://user-images.githubusercontent.com/96654391/201233775-be37fa34-e78c-4251-81e2-1ab240cdbe65.png">

### Network Design

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/201234083-d698b794-8f71-40b1-acee-d8429e235f74.png"> <br>

주로 사용한 모델의 구조는 GoogleNet. Conv Layer를 많이 쌓으면 연산량이 증가해서 중간에 Reduction Layer를 추가한 것을 확인할 수 있다.

### Training Stage

어떻게 학습을 하는지에 대해서 다룸. <br>

<img width="1000" alt="download2" src="https://user-images.githubusercontent.com/96654391/201234600-ef0d46a3-12fd-48b8-a5f1-b6db20c8b5be.png"> <br>
실제 객체가 있는 박스를 Groundtruth라고 하는데, 여기서 박스의 중앙에 점을 포함하고 있는 셀이 responsible한 셀이 되는 것이다. <br>

<img width="1000" alt="download3" src="https://user-images.githubusercontent.com/96654391/201234827-00c45019-3779-414c-bb94-8995413e4344.png"> <br>
학습을 할 때에는 한 가지 BBox를 사용을 하고, 여기서 BBox가 결정되는 기준이 IoU이다. Groundtruth와 겹치는 부분이 가장 많은 박스로 선택. <br>
-> IoU가 가장 큰 값을 가진 박스는 노란색 박스이고, 여기서 스칼라값을 1로 표시하여 Loss Function에 반영이 되게 하였고, 다른 스칼라값은 0으로 두어 Loss Function에 반영이 되지 않게 하였다. <br>
<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/201235276-195f4616-381f-4bf6-8460-a89daaa464e8.png"> <br>
첫 번째는 모든 셀에 대해서 B개(예측하고자 했던 BBox 개수)의 BBox 좌표와 GT(GroundTruth) Box좌표의 오차를 구하는 공식 <br>

두 번째를 모든 셀에 대해서 B개의 확률값과 GT값의 오차를 구하는 공식 <br>

모든 셀의 confidence score와 정답 값의 오차를 구하는 공식 <br>

첫 번째 스칼라값: 몇 번째의 셀이 가장 예측력이 좋은 BBox를 표현 <br>

두 번째 스칼라값: 물체가 나타났는지(1) 아닌지(0)를 표현 <br>

세 번째 람다값: 어떤 BBox의 손실을 더 반영을 할 것인지를 표현 <br>

즉, 그리드 셀에 객체가 존재하는 경우의 오차와 예측 박스로 선정된 경우에만 오차를 학습한다.

### Inference Stage(예측 단계)

첫 번째 BBox의 클래스 확률값과 학습 과정에서 구했던 Class Probability를 곱해서 각 바운딩 박스마다 구하게 된다. <br>

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/201236990-7217b720-f8cd-4ae3-8a48-1cdc2321b936.png"> <br>
이 과정대로 진행을 하게 된다면 BBox의 개수가 많아지게 되어, NMS(Non-Maximum Suppression) 알고리즘이 적용된다. <br>

<img width="1000" alt="download2" src="https://user-images.githubusercontent.com/96654391/201237347-f8f1f5d6-d0be-4f0a-9e1e-514834f39d47.png"> <br>
이 알고리즘을 통해 클래스 별로 비교를 해서 가장 예측력이 좋은 BBox만 남길 수 있다. 코드를 보면 특정 값을 넘지 않으면 0으로 처리 <br>
<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/201237950-ba0a9dc6-b2ca-48ae-a27b-d4742f38c11e.png"> <br>
-> 예측값이 높은 BBox를 기준으로 내림차순 정렬 <br>

나머지 BBox는 BBox#12와의 IoU가 높아서 NMS에 의해서 제거된다. <br>

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/201238783-35c1d524-8c25-4633-bdfd-43960019e090.png"> <br>
만약 탐지해야되는 객체가 여러 개일 경우, 예측값이 높은 BBox끼리 IoU를 비교를 할 텐데, 이 때의 IoU는 낮은 값 또는 0의 값을 가지게 되므로 NMMS에 의해서 제거되지 않는다. <br>

서로 다른 Object가 있을 때 <br>

<img width="1000" alt="download2" src="https://user-images.githubusercontent.com/96654391/201240003-c59fe056-7915-4927-9dae-1c0344592b7f.png">

### 성능

속도: Fast YOLO >> YOLO >> DPM, R-CNN <br>
성능: Faster R-CNN >> Fast R-CNN >> YOLO >> DPM

### Limitaion

1. 작은 물체에 대해서 탐지 성능이 낮음 -> BBox가 작게 되어서 IoU의 값 차이가 작아지게 되기 때문에
2. 일반화된 지식이랑 다르게 객체 비율이 달라지면 detection 성능이 낮아짐.

## YOLO9000: Better, Faster, Stronger

### Main Contribution

- YOLOv2제안: YOLOv1의 단점을 개선하여 연산을 빠르게 정확도는 높임
- YOLO9000제안: Detection dataset의 적은 Class 개수로 인한 예측 가능한 class 개수의 증가. -> 존재하지 않은 클래스에 대한 예측도 가능해짐
- 새로운 classification network인 Darknet-19를 통해서 성능 향상

### Better

1. batch normalization <br>
   이전 layer의 파라미터 변화로 인해 현재 layer의 입력 분포가 바뀌는 현상을 방지하기 위해서 사용. -> mini batch 사용하여 학습 시, 빠른 수렴 가능 + 정규화 효과를 통해서 overfitting 발생 X

2. high resolution classifier <br>
   YOLOv1에서는 classification에서는 저해상도를 사용해서 pretrain을 시킨다. 그 이후 task에서는 고해상도 사진을 넣고 Fine tuning 하는 과정을 거쳤는데, 이렇게 된다면 네트워크가 피팅되는데 시간이 걸린다. <br>
   <img width="700" alt="download1" src="https://user-images.githubusercontent.com/96654391/201248498-a3d1eb63-bb80-480a-acbb-2193b7de2203.png"> <br>
   Sol. 똑같이 저해상도 이미지로 pretrain을 한 다음 마지막 10 에포크 정도를 고해상도 이미지로 Fine-tunning 하게 한다.

3. Convolutional with anchor boxes <br>

   <img width="700" alt="download2" src="https://user-images.githubusercontent.com/96654391/201249038-c44a4c9c-674a-43b5-a11b-bbe3cb3e5330.png"> <br>
   <center>YOLOv2와 v1과의 차이</center> <br>

   <img width="518" alt="download3" src="https://user-images.githubusercontent.com/96654391/201249825-c5cd03f2-67b4-4be0-8782-d3741e07056b.png"> <br>
   그리드 별로 anchor box 5개를 예측하되 anchor box의 원점은 grid cell 내에 존재하도록 예측해야된다. <br>
   output의 경우는 v1과 마찬가지고 x, y, w, h, Pc(클래스 예측 확률값)으로 나온다.

4. dimension clusters
   18분 부터
