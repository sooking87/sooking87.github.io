---
title: "[week 2] Part 1: Basic Machine Learning (2)"
excerpt: "[week 2] Part 1: Basic Machine Learning (2)"
categories: [GDSC ML]
tags: [GDSC ML, Python]
toc: true
toc_sticky: true
---

## Multi-variable linear regression

- Hypothesis
- Cost function: 가설과 실제값의 제곱의 평균. Cost를 최소화하는 W(기울기)를 찾아가는 것이 포인트
- Gradient descent: Cost를 최소화하는 W를 찾을 수 있는 알고리즘 중 하나 <br>

변수가 여러개가 된다면 이에 따른 가중치도 여러개가 된다. 예를 들어서 변수가 3개라면 가중치(W)도 3개가 된다. <br>

변수가 여러개가 된다면 Matrix 곱셈(행렬곱)을 활용하게 된다.

<img width="453" alt="download1" src="https://user-images.githubusercontent.com/96654391/194480705-e36c51d6-8977-454e-9a2b-82b0cbbba918.png"> <br>

일반적으로 matrix는 대문자로 표현을 한다. 매트릭스 연산 시 X가 먼저, W가 뒤에 오기 때문에 기술을 할때도 XW라고 한다. X를 사용하게 된다면 컬럼의 개수가 몇 개든, 데이터의 개수가 몇 개든 상관이 없이 무조건 X로 사용하기 때문에 간단하게 식 표현이 가능하다.

## Multi variable linear regression LAB

### Multi variable linear regression with no matrix

```py
# Multi variable linear regression with no matrix
import tensorflow as tf

# data and label
x1 = [73., 93., 89., 96., 73]
x2 = [80., 88.,91., 98., 66.]
x3 = [75., 93., 90., 100., 70.]
Y = [152., 185., 180., 196., 142.]

# weights
w1 = tf.Variable(tf.random.normal([1]))
w2 = tf.Variable(tf.random.normal([1]))
w3 = tf.Variable(tf.random.normal([1]))
b = tf.Variable(tf.random.normal([1]))

learning_rate = 0.000001

hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b

for i in range(1000+1):
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b
        cost = tf.reduce_mean(tf.square(hypothesis - Y))
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])

    # update w1, w2, w3 and b
    w1.assign_sub(learning_rate * w1_grad)
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:
        print("{:5} | {:12.4f}".format(i, cost.numpy()))

>>>
    0 |   21359.2578
   50 |     245.4268
  100 |      11.1252
  150 |       8.5033
  200 |       8.4521
  250 |       8.4294
  300 |       8.4072
  350 |       8.3851
  400 |       8.3629
  450 |       8.3410
  500 |       8.3189
  550 |       8.2970
  600 |       8.2751
  650 |       8.2533
  700 |       8.2315
  750 |       8.2098
  800 |       8.1882
  850 |       8.1667
  900 |       8.1451
  950 |       8.1236
 1000 |       8.1022
```

칼럼의 개수만큼 가중치를 따로따로 정의해주어야 됨을 알 수 있다.

### Multi variable linear regression with matrix

```py
# Multi variable linear regression with matrix
import numpy as np

data = np.array([
    # x1, x2, x3, y
    [73., 80., 75., 152.],
    [93., 88., 93., 185.],
    [89., 91., 90., 180.],
    [96., 98., 100., 196.],
    [73., 66., 70., 142.],
], dtype=np.float32)

# slice data
X = data[:, :-1]
y = data[:, [-1]]

# 가중치의 경우 칼럼이 3개(x1, x2, x3)니까 행은 3개가 필요하고, 결과값은 1개니까 열 개수는 1개가 필요하다.
W = tf.Variable(tf.random.normal([3, 1]))
b = tf.Variable(tf.random.normal([1]))

learning_data = 0.000001

def predict(X):
    return tf.matmul(X, W) + b

n_epochs = 2000
for i in range(n_epochs + 1):
    # record the gradient of the cost function
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])

    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 100 == 0:
        print("{:5} | {:10.4f}".format(i, cost.numpy()))

>>>
    0 | 33753.4961
  100 |    14.9227
  200 |    10.7120
  300 |    10.6551
  400 |    10.5989
  500 |    10.5431
  600 |    10.4877
  700 |    10.4324
  800 |    10.3776
  900 |    10.3230
 1000 |    10.2687
 1100 |    10.2147
 1200 |    10.1609
 1300 |    10.1075
 1400 |    10.0543
 1500 |    10.0015
 1600 |     9.9489
 1700 |     9.8966
 1800 |     9.8447
 1900 |     9.7929
 2000 |     9.7415
```

행렬을 사용하게 된다면 가설 함수의 결과값을 구할 때, 훨씬 간편하게 코드를 작성할 수 있음을 확인할 수 있다.

## Logistic (Regression) Classification

출처: <https://copycode.tistory.com/161?category=740659> + 스터디 강의

### Classification

- Exam: Pass or Fail
- Spam: Not Spam or Spam
- Face: Real or Fake
- Tummor: Not Malignant or Malignant <br>

이게 다 Binary Classification이다. 어떤 피처의 특징을 토대로 T / F로 나눌 수 있고 이를 통해서 Logistic Regression

### Logistic vs Linear

- Logistic: 두가지의 케이스로 구분할 수 있다. Show size / The number of workers
- Linear: 연속적인 데이터를 통해서 그래프에 인근하는 데이터를 예측할 수 있다. Time / Weight / Height 처럼 연속적인 값들

### Hypothesis Representiation

앞에서 배웠던 Linear Regression을 가지고 0.5보다 크면 1로 작으면 0으로 하면 될 것 같다는 생각이 든다. 하지만 이 알고리즘을 하용하게 된다면 특정 데이터가 크게 나온다면 원래는 1을 받을 수 있지만 그 데이터가 0을 받을 수 있다. 정확한 데이터를 표현하지 못하게 될 수 있다. 그렇기 때문에 Sigmod 함수를 사용해서 해당 데이터를 통해서 나온 Linear Regression을 0에서 1사이의 값으로 바꾼다. <br>

Linear Regression을 통해서 나온 실수값을 Logistic Function(Sigmoid)을 통해서 0에서 1인 구간을 만들고 Decision Boundary(0과 1을 구별하는 함수)를 통해서 0.5이상이면 1로 값을 변경시키는 방식으로 표현한다. 이게 Logistic Hypothesis가 된다.

### Cost Function

Cost function은 예측을 하는 값과 실제 값의 차이를 나타내는 함수이다. Linear Regression의 Cost Function의 경우는 실제 값들과의 차이에 대한 기울기를 이용해서 만들어지게 된다. <br>

하지만 이 Cost Function 공식을 사용하게 된다면 Logistic Regression의 경우는 울퉁불퉁한 값이 나와서 Gradient descent algorithm을 통해서 최솟값을 구할 수 없게 된다.

![download1](https://user-images.githubusercontent.com/96654391/194720905-0179169a-1864-4f24-b3da-8191a3072ac8.png)<br>

![download1](https://user-images.githubusercontent.com/96654391/194720905-0179169a-1864-4f24-b3da-8191a3072ac8.png)<br>

local cost의 최소값과 Global cost의 최솟값은 다를 수 있기 때문이다. <br>

따라서, Logistic Regression의 cost function은 다음과 같이 정의할 수 있다.

![download2](https://user-images.githubusercontent.com/96654391/194721121-f7485e8f-845e-429b-ad89-3718c5828f15.png) <br>

왜냐면 가설 함수에서 시그모이드를 통해서 0~1로 바꾸어 놓았기 때문에 log를 사용하게 된다면 확실히 0인지 1인지를 확인할 수 있기 때문이다. 예를 들어서 1로 수렴할수록 log를 가지고 있는 함수에 대한 y값은 0에 수렴하게 된다. 즉, 예측값이 1에 가까워질수록 cost function의 값은 작아지게 된다. 반대로 y에 가까워진다고 해도, -log에 해당한다면 그 cost 역시 작아지게 된다.

![download1](https://user-images.githubusercontent.com/96654391/194721245-5290e055-ccdb-47dc-a67a-fe079c8fb743.png) <br>

따라서 Logistic Regression의 Cost Function은 다음과 같은 식을 가집니다.

![download2](https://user-images.githubusercontent.com/96654391/194721298-44e55f81-2f67-4208-9e67-8c2d5211737e.png)
<br>

### How to minimize the cost function

그럼 위에 있는 Cost Function에서 비용의 최소화되는 값을 어떻게 알 수 있을까?

![download1](https://user-images.githubusercontent.com/96654391/194721519-3700c2b0-63c1-4256-b540-ab1b2141b5a2.png) <br>

Cost Function을 구한 이유는 Gradient descent algorithm을 사용하기 위해서이다.
