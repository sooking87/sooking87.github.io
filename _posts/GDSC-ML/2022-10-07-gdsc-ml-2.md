---
title: "[week 2] Part 1: Basic Machine Learning (2)"
excerpt: "[week 2] Part 1: Basic Machine Learning (2)"
categories: [GDSC ML]
tags: [GDSC ML, Python]
toc: true
toc_sticky: true
---

## Multi-variable linear regression

- Hypothesis
- Cost function: 가설과 실제값의 제곱의 평균. Cost를 최소화하는 W(기울기)를 찾아가는 것이 포인트
- Gradient descent: Cost를 최소화하는 W를 찾을 수 있는 알고리즘 중 하나 <br>

변수가 여러개가 된다면 이에 따른 가중치도 여러개가 된다. 예를 들어서 변수가 3개라면 가중치(W)도 3개가 된다. <br>

변수가 여러개가 된다면 Matrix 곱셈(행렬곱)을 활용하게 된다.

<img width="453" alt="download1" src="https://user-images.githubusercontent.com/96654391/194480705-e36c51d6-8977-454e-9a2b-82b0cbbba918.png"> <br>

일반적으로 matrix는 대문자로 표현을 한다. 매트릭스 연산 시 X가 먼저, W가 뒤에 오기 때문에 기술을 할때도 XW라고 한다. X를 사용하게 된다면 컬럼의 개수가 몇 개든, 데이터의 개수가 몇 개든 상관이 없이 무조건 X로 사용하기 때문에 간단하게 식 표현이 가능하다.

## Multi variable linear regression LAB

### Multi variable linear regression with no matrix

```py
# Multi variable linear regression with no matrix
import tensorflow as tf

# data and label
x1 = [73., 93., 89., 96., 73]
x2 = [80., 88.,91., 98., 66.]
x3 = [75., 93., 90., 100., 70.]
Y = [152., 185., 180., 196., 142.]

# weights
w1 = tf.Variable(tf.random.normal([1]))
w2 = tf.Variable(tf.random.normal([1]))
w3 = tf.Variable(tf.random.normal([1]))
b = tf.Variable(tf.random.normal([1]))

learning_rate = 0.000001

hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b

for i in range(1000+1):
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b
        cost = tf.reduce_mean(tf.square(hypothesis - Y))
    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])

    # update w1, w2, w3 and b
    w1.assign_sub(learning_rate * w1_grad)
    w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 50 == 0:
        print("{:5} | {:12.4f}".format(i, cost.numpy()))

>>>
    0 |   21359.2578
   50 |     245.4268
  100 |      11.1252
  150 |       8.5033
  200 |       8.4521
  250 |       8.4294
  300 |       8.4072
  350 |       8.3851
  400 |       8.3629
  450 |       8.3410
  500 |       8.3189
  550 |       8.2970
  600 |       8.2751
  650 |       8.2533
  700 |       8.2315
  750 |       8.2098
  800 |       8.1882
  850 |       8.1667
  900 |       8.1451
  950 |       8.1236
 1000 |       8.1022
```

칼럼의 개수만큼 가중치를 따로따로 정의해주어야 됨을 알 수 있다.

### Multi variable linear regression with matrix

```py
# Multi variable linear regression with matrix
import numpy as np

data = np.array([
    # x1, x2, x3, y
    [73., 80., 75., 152.],
    [93., 88., 93., 185.],
    [89., 91., 90., 180.],
    [96., 98., 100., 196.],
    [73., 66., 70., 142.],
], dtype=np.float32)

# slice data
X = data[:, :-1]
y = data[:, [-1]]

# 가중치의 경우 칼럼이 3개(x1, x2, x3)니까 행은 3개가 필요하고, 결과값은 1개니까 열 개수는 1개가 필요하다.
W = tf.Variable(tf.random.normal([3, 1]))
b = tf.Variable(tf.random.normal([1]))

learning_data = 0.000001

def predict(X):
    return tf.matmul(X, W) + b

n_epochs = 2000
for i in range(n_epochs + 1):
    # record the gradient of the cost function
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])

    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad)

    if i % 100 == 0:
        print("{:5} | {:10.4f}".format(i, cost.numpy()))

>>>
    0 | 33753.4961
  100 |    14.9227
  200 |    10.7120
  300 |    10.6551
  400 |    10.5989
  500 |    10.5431
  600 |    10.4877
  700 |    10.4324
  800 |    10.3776
  900 |    10.3230
 1000 |    10.2687
 1100 |    10.2147
 1200 |    10.1609
 1300 |    10.1075
 1400 |    10.0543
 1500 |    10.0015
 1600 |     9.9489
 1700 |     9.8966
 1800 |     9.8447
 1900 |     9.7929
 2000 |     9.7415
```

행렬을 사용하게 된다면 가설 함수의 결과값을 구할 때, 훨씬 간편하게 코드를 작성할 수 있음을 확인할 수 있다.
