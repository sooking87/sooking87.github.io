---
title: "[week 5] Object Detection: 1-stage-detector"
excerpt: "[week 5] Object Detection: 1-stage-detector"
categories: [GDSC ML]
tags: [GDSC ML, Python]
toc: true
toc_sticky: true
---

## Semantic Segmentation

의미론적 분할이라는 것은 이미지를 넣었을 때, 모든 픽셀 이미지의 카테고리를 분류해 주는 것을 말한다. <br>

<img width="400" alt="download1" src="https://user-images.githubusercontent.com/96654391/199552432-0ce8348a-51c6-43ef-af33-dbad12a2ad72.png"> <br>

하지만 붙어있는 객체라면 하나의 객체로 판별을 하게 된다.

### idea 1: sliding Window

<img width="1000" alt="download2" src="https://user-images.githubusercontent.com/96654391/199552700-f0f8e4a0-92c1-4084-afe8-d5184b4aabda.png"> <br>

이미지가 들어온다면 그 이미지를 매우 작게 쪼갠다. 그 다음 작은 이미지가 어떤 카테고리에 들어갈지 일종의 분류 문제로 취급하게 된다. <br>

이렇게 분류를 한다면 분류는 되겠지만 그렇게 좋은 아이디어는 아니다. <br>

1. 엄청난 계산 비용(픽셀들을 순방향, 역방향으로 계산해서 카테고리 분류를 하기 때문)
2. 같은 객체지만 붙어있는 경우 처리할 수 없다. <br>

그래서 아무도 사용은 하지 않음. 하지만 Semantic Segmentation이 무엇인지는 알 수 있게 한다.

### idea 2: Fully Convolutional

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/199553806-6c8c63ee-505a-4919-b2d0-201f4fd7a599.png">

이미지가 들어가게 된다면 깊은 Conv 레이더들을 거치면서 한 번에 픽셀들을 분류를 할 수 있다. 그래서 각 층별로 특정 카테고리에 대한 점수를 통해서 카테고리를 분류할 수 있다. 따라서, 마지막 output의 깊이는 카테고리의 개수와 동일해야 된다. <br>

sliding Window 보다는 나은 점 <br>

이미지를 작게 쪼개서 하나하나 독립적으로 계산을 하는게 아니라 한 번에 네트워크를 통해서 픽셀 별 카테고리를 분류하는 것이므로 sliding Window보다는 계산이 간단하다. <br>

문제점 <br>

1. 엄청난 계산 비용이 든다. <br>

따라서 위 사진의 있는 네트워크는 사용하지 않지만, 아래 사진에 있는 네트워크는 사용하는 경우가 있다. <br>

<img width="613" alt="download1" src="https://user-images.githubusercontent.com/96654391/199557905-8dcd4c32-4ce7-4cd4-9373-a4c66e0176c7.png"> <br>

네트워크 안에서 downsampling, upsampling을 통해서 사용하는 구조도 있다. downsampling의 방법에서는 CNN에 대해서 배울 때 많이 배웠다. Pooling, strided convolution(stride를 늘려서 크기를 줄이는 conv)를 통해서 downsampling이 가능하다.

#### UpSampling

downsampling하는 방법에 대해서는 배웠지만 upsampling 하는 방법에 대해서는 배우지 않았으므로 upsampling 하는 과정에 대해서 배운다.

1. Unpooling <br>

   풀링의 반대 과정, 맥스 풀링과 평균 풀링이 있었던 것 처럼 unpooling도 크게 2가지 방식이 있다.

   <img width="800" alt="download1" src="https://user-images.githubusercontent.com/96654391/199564409-4633bf8d-f7d9-40e1-8790-af865f09e9b1.png"> <br>

   여기서 Bed of Nails 방식에서 좀더 나아가 Max Unpooling 방식이 있다. <br>

   <img width="800" alt="download2" src="https://user-images.githubusercontent.com/96654391/199565041-47b6aa1f-7beb-4846-a5bf-264c4d5a1bf6.png"> <br>

   이 네트워크의 경우는 절반은 donwsampling을 하고 대칭적으로 upsampling을 하기 때문에 pair를 맞춰서 해당 맥스 풀링에 기여했던 레이어의 위치에 non-zero 값을 넣는 것이다.

2. Transpose Convolution

왜 Transpose Conv일까? <br>

원래 합성곱 연산 ⏬ <br>

<img width="800" alt="download1" src="https://user-images.githubusercontent.com/96654391/199570081-7a5edcc6-a170-4901-aa52-72d10c62406d.png"> <br>

Transpose Convolution 연산 ⏬ <br>

<img width="800" alt="download2" src="https://user-images.githubusercontent.com/96654391/199570090-b21084e2-5951-47b2-89b2-9293bd71cd9d.png"> <br>

여기에서 필터를 transpose시켜서 transpose 된 C X output을 곱해서 input이었던 크기로 복구를 시킬 수 있다. 이 과정에서 필터와 output의 곱이 overlap된다면 그 값은 더해준다. <br>

Transpose Convolution <br>
= Deconvolution <br>
:: 논문에서는 볼 수 있지만 그닥 좋지 않은 의미이다. 신호 처리 관점에서는 deconvolution이라는 것 자체가 사실은 convolution의 역연산의 의미이이긴 하다. <br>

= Upconvolution <br>
= Fractionally strided convolution <br>
= Backward strided convolution <br>

## Classification + Localization

이미지 안의 객체를 분류해야되고 이와 함계 위치도 판별해야된다. <br>

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/199760690-9d9c1943-6226-4f07-bb09-e4fb2556fa62.png"> <br>

- 첫 번째 FC: Classification -> output: set of Class Score -> output과 Correct Label의 Loss -> softmax Loss
- 두 번재 FC: Localization -> output: four numbers(bounding box's x, y, w, h) -> output과 Correct output의 Loss -> L2(L1, smooth L1) Loss <br>

softmax Loss + L2 Loss = Loss를 사용한다. <br>

<img width="1000" alt="download2" src="https://user-images.githubusercontent.com/96654391/199767898-c7dd987b-a985-4928-a1f5-695e21842f45.png"> <br>

적용 예시 ⏫ <br>

사람의 joints를 기준으로 14개로 나누어서 14개의 x, y 예측 지점에 대해서 14개의 regression Loss(ex. L2 Euclidean loss, L1 loss 등)을 적용하고 back propagation을 통해서 다시 훈련을 한다. <br>

자세 예측을 통해서 각각 14개의 x, y가 나왔는데 이를 통해서 classification을 여러 개 하더라도 이에 맞는 bounding box를 여러 개 output할 수 있어서 다양한 문제에 적용될 수 있다. (Object Detection)

## Object Detection

Classification + Localization와의 차이점은 각 이미지에 대해서 object가 다를 수 있기 때문에 예측 해야되는 개수가 다르다. 이미지마다 객체의 개수가 다르고 이를 예측하기 어려우므로 <br>

Object Detection으로 regression을 이용하기에는 어렵다.

### Sliding Window

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/199777067-b0c4ddbc-8460-4b8d-96db-3b8e128f2804.png"> <br>

Semantic Segmentation와 비슷하게 모든 crop들을 분석하여 카테고리 별로 분류하는 형식이다. <br>

문제점 <br>

이미지의 객체가 모든 위치에 나타날 수 있고, 크기도 다 다르고, w, h도 다 다르기 때문에 이렇게 무차별적인 sliding window를 하려면 너무 많은 게산이 소모된다.

### Region Proposals

blobby regions를 찾은 다음 give some set of candidate proposal regions(object가 있을 수 있는) <br>

selective search의 경우는 약 2000 개의 proposals regions를 제공한다. 따라서 어디에 있을 모르는 object를 분류하는 것 대신에 Region Proposal Networks를 사용해서 object가 있을 만한 영역을 통해서 convolution network를 통해서 classification을 한다. <br>

이 아이디어는 R-CNN을 통해서 왔다.

## R-CNN

<img width="1000" alt="download2" src="https://user-images.githubusercontent.com/96654391/199777076-9778762d-9127-45f8-80f5-a1cfdd43157e.png"> <br>

1. Region Proposal Network를 통해서 나온 영역을 Regions of Interest(ROI)라고 부른다. 이 ROI에 대해서 다시 selective search를 통해서 2000개의 ROI를 받는다.
2. Convolution Network에 넣기 전에 크기를 조정해서 width와 height를 동일하게 한다.
3. Convolution Network
4. 분류하기 위해서 SVM을 사용 + bounding box의 4가지 값도 예측 <br>

문제점 <br>

- 많은 영역을 훈련시켜야됨 -> expensive
- slow

## Fast R-CNN

R-CNN과 유사하지만 ROI를 개별적으로 처리하는 것이 아니라 전체 이미지에 해당하는 것을 처리한다. <br>

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/199781665-f285054c-b094-44ef-a859-fcd3c1e1b39c.png">

1. 전체 이미지 -> ConvNet :: to give this high resolution convolutional feature map correspoding entire image
2. Regions Proposal method :: projecting thos region proposals -> taking crops
3. RoI Pooling layer
4. FC -> output: classification softmax, bbox regression
5. back propagation learn jointly

<img width="1000" alt="download2" src="https://user-images.githubusercontent.com/96654391/199781675-3bdc673e-7265-45ff-8cd7-c47aed54dc5f.png">

<center>다른 사진 Layer</center>

문제점 <br>

fixed function을 사용해서 bottleneck 발생

## Faster R-CNN

Fast R-CNN의 문제점을 네트 워크 자체가 own region proposas를 예측하도록 하여 해결하였다. <br>

구성: Region Proposal Network -> Fast R-CNN <br>

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/199792503-f7c2eca3-5ce3-4aa9-a774-358d8357c774.png">

전체 이미지를 CNN을 통해서 나온 feature map 에서 RPN을 통해서 개별적으로 RoI를 결정한다. 그 다음부터는 Fast R-CNN과 동일. <br>

Region Proposal Network needs to do two things -> object인지 아닌지 여부 파악(binary classification), regress bbox <br>

Final Network needs to do two things -> 클래스 점수에 대한 분류, regress bbox to again correct any errors

## Detection without Proposals: YOLO / SSD

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/199806074-de166493-ba50-4e7b-b75c-65d7fa86802e.png">

1. 예측하고자 하는 이미지를 7x7 그리드 셀로 나누고 각 cell 마다 하나의 객체를 예측
2. boundary boxes를 통해서 객체의 위치와 크기 파악
   - B개의 boundary boxes를 예측하고 각 box는 하나의 box confidence score를 가지고 있다. box confidence score는 box가 객체를 포함하고 있을 가능성(objectness)과 boundary box가 얼마나 정확한지를 반영한다.
   - C개의 conditional class probabilities를 예측한다. Conditional class probabilities는 탐지된 객체가 어느 특정 클래스에 속하는지에 대한 확률이다.
3. confidence score순으로 예측을 정렬
4. 제일 높은 score에서 시작해서, 이전의 예측과 클래스가 같고 IOU > 0.5인 것이 있으면 현재의 예측 무시
5. 모든 예측을 확인할 때까지 4번 과정 반복

## Instance Segmentation

object detection + detect whole segmentation mask <br>

<img width="1000" alt="download1" src="https://user-images.githubusercontent.com/96654391/199807483-5c94e63a-e678-4851-87f1-b19ae2fa4f41.png"> <br>

whole input image -> CNN -> learned RPN -> warp -> two branches

1. faster R-CNN -> category, bbox
2. object 여부 상관없이 픽셀에 대해서 분류 -> sementation
