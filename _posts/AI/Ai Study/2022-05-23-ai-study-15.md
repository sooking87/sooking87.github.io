---
title: "[chap 7-1] 인공 신경망"
excerpt: "[chap 7-1] 인공 신경망"
categories: [Ai Study]
tags: [Ai Study, Python]
toc: true
toc_sticky: true
---

📌 7장과 8장은 패션 MNIST 데이터셋을 사용할 예정

### 📍 문제점

럭키백 이벤트를 20대에 초점을 맞추어 패션 분야 이벤트를 진행하려고 한다. -> HOW?

## 1️⃣ 사용할 데이터 : 패션 MNIST 데이터셋

```python
from tensorflow import keras
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

print(train_input.shape, train_target.shape) # (60000, 28, 28) (60000,)
print(test_input.shape, test_target.shape) # (10000, 28, 28) (10000,)

import matplotlib.pyplot as plt
fig, axs = plt.subplots(1, 10, figsize=(10, 10))
for i in range(10):
  axs[i].imshow(train_input[i], cmap='gray_r')
  axs[i].axis('off')
plt.show()
```

![download1](https://user-images.githubusercontent.com/96654391/169847205-1ae5a8c6-ce82-4b99-8fb3-c313d45f2d25.png) <br>

이런 식으로 아이템들이 있다. 레이블이 0~9까지 있고 각각 레이블에 맞는 패션 아이템이 지정되어 있다.

## +a. 인공 신경망

- 출력층 : 신경망의 최종 값
- 뉴런 / 유닛 : 출력층의 z값을 계산하는 단위(입력층에서의 각각의 픽셀들과 출력층에 있는 10개 모두 뉴런임)
- 입력층 : 입력값(위 패션 MNIST의 경우는 28 \* 28개의 픽셀이 입력층) <br>

-> 입력층 \* 가중치 = 출력층 결정 <br>

그래서 인공 신경망이란 생물학적 뉴런에서 본떠서 만든 개념으로, 뇌 속에 있는 무언가를 만들어가는 일이다.

## +a. 텐서플로와 케라스

- 텐서플로 : 딥러닝 라리브러리로 저수준 API와 고수준 API가 있다.
- 케라스 : 텐서플로의 고수준 API <br>

딥러닝 라이브러리는 다른 머신러닝 라이브러리와는 다르게 그래픽 처리 장치는 GPU를 사용하여 인공 신경망을 훈련합니다. 이때 케라스 라이브러리는 직접 GPU 연산을 수행하지 않습니다. 여기서 GPU 연산을 수행하는 다른 라이브러리를 **백엔드** 로 사용한다. -> 텐서플로는 케라스의 백엔드(계산 처리 장치)

## 2️⃣ 인공 신경망으로 모델 만들기

로지스틱 회귀에서는 교차 검증을 사용해 모델을 평가하지만, 인공 신경망에서는 교차검등을 사용하지 않고 별도로 검증 세트를 덜어내어 사용한다. -> WHY?

<br>

1. 딥러닝 분야의 데이터셋은 충분히 크다.
2. 교차 검증을 수행하기에는 훈련 시간이 오래걸린다.

```python
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)
print(train_scaled.shape, train_target.shape)
print(val_scaled.shape, val_scaled.shape)

>>>
(38400, 784) (38400,)
(9600, 784) (9600, 784)
```

- 🚦 train_scaled, train_target : 훈련 세트(80%)
- 🚦 val_scaled, val_target : 검증 세트(20%)

<br>

케라스의 레이어 패키지에는 다양한 층이 준비되어 있는데, 가장 기본이 되는 층은 밀집층이다. -> 왼쪽의 뉴런과 오른쪽 뉴런이 연결된 경우를 밀집층 또는 완전 연결층이라고 부른다.

```python
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784, ))
```

keras.layers.Dense 클래스의 매개변수

- 첫 번째 매개변수 : 뉴런 개수
- activation='softmax' : 10개의 뉴런에서 출력되는 값을 확률로 바꾸기 위해서는 소프트맥스 함수를 사용 -> 다중분류에서는 소프트맥스함수를 사용하고, 이진분류에서는 시그모이드 함수를 사용(activation='sigmoid')
- input_shape=(784, ) : 입력의 크기

### 📍 밀집층을 가진 신경망 모델 만들기

=> 케라스의 Sequential 클래스 사용하기

```python
model = keras.Sequential(dense)

```

- model : 신경망 모델
- dense : 밀집층의 객체 <br>

이 번 장에서 배운 활성화 함수 : 소프트맥스 함수. 활성화 함수란 뉴런의 선형 방정식 계산 결과에 적용되는 함수이다. 즉 10개의 출력층에서 소프트맥스 함수를 통해서 선형 방정식의 결과가 나오는데, 이렇게 된다면 이 장에서의 활성화 함수는 소프트맥스 함수이다.

## 3️⃣ 인공 신경망으로 패션 아이템 분류하기

-> 훈련 전 설정 단계 : compile() 메소드

```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

model.fit(train_scaled, train_target, epochs=5)

>>>
Epoch 1/5
1500/1500 [==============================] - 7s 3ms/step - loss: 0.6082 - accuracy: 0.7943
Epoch 2/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.4785 - accuracy: 0.8390
Epoch 3/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.4564 - accuracy: 0.8475
Epoch 4/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.4447 - accuracy: 0.8520
Epoch 5/5
1500/1500 [==============================] - 4s 3ms/step - loss: 0.4381 - accuracy: 0.8553
<keras.callbacks.History at 0x7f0a80652890>
```

4장에서 로지스틱 손실 함수로 _이진 크로스 엔트로피 손실 함수_ 를 사용했었다. -> 왜? 예측 확률의 손실 정도를 파악하여서 모델의 정확도 점수를 판단하는데 사용함. <br>

케라스도 마찬가지로 손실 함수를 사용한다.

- 이진 분류 : loss = 'binary_crossentropy'
- 다중 분류 : loss = 'categorical_crossentropy'

<br>

케라스에서 이진 분류인 경우 **_손실 함수 값 \* 타깃값_** 을 하게 되어 양성 샘플의 결과만 1이 나오도록 해준다. <br>

케라스에서 다중 분류인 경우 타깃에 해당하는 확률만 남겨 놓기 위해서 나머지 확률에는 모두 0을 곱한다. 그래서 예를 들어서 티셔츠 샘플이라고 한다면 출력층에서 나온 확률이 티셔츠일 확률을 제외한 나머지 확률들을 모두 0을 곱한다. -> 타깃값을 해당 클래스만 1이고, 나머지는 모두 0인 배열로 만드는 것을 **_원 핫 인코딩_** 이라고 부른다.

<br>

패션 MNIST 데이터에 크로스 엔트로피를 적용하려면 사실은 [1, 0, 0, 0, ... 0] 이런식으로 배열을 만들어야 하지만 텐서플로에서는 정수로 된 타깃값을 원-핫 인코등으로 바꾸지 않고 사용할 수 있고, 이때 정수로 된 타깃값을 사용해 크로스 엔트로피 손실을 계산하는 것이 바료 **sparse_categorical_crossentropy** 이다. <br>

그리고 compile() 메소드의 두 번째 metrics 라는 매개변수에 accuracy를 지정하여 정확도까지 함계 출력되도록 함.

## 🌗 케라스에서 모델 성능 평가

```python
model.evaluate(val_scaled, val_target)

375/375 [==============================] - 1s 3ms/step - loss: 0.4402 - accuracy: 0.8511
[0.44022828340530396, 0.8510833382606506]
```

- evaluate(val_scaled, val_target) -> 85% 정도 나오는 것을 확인할 수 있다.
