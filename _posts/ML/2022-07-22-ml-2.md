---
title: "[chap 2] ì‚¬ì´í‚·ëŸ°ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹"
excerpt: "[chap 2] ì‚¬ì´í‚·ëŸ°ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹"
categories: [ML]
tags: [ML, Python]
toc: true
toc_sticky: true
---

# ë¶ˆê½ƒ í’ˆì¢… ì˜ˆì¸¡í•˜ê¸°

## 1ï¸âƒ£ load_iris ê°€ì ¸ì˜¤ê¸°

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import pandas as pd

# ë¶ˆê½ƒ ë°ì´í„° ì…‹ ë¡œë”©
iris = load_iris()

# iris.dataëŠ” Iris ë°ì´í„° ì„¸íŠ¸ì—£ í”¼ì²˜ë§Œìœ¼ë¡œ ëœ ë°ì´í„°ë¥¼ ë„˜íŒŒì´ë¡œ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
iris_data = iris.data

# iris.targetì€ ë¶ˆê½ƒ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ë ˆì´ë¸” ë°ì´í„°ë¥¼ ë„˜íŒŒì´ë¡œ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤ .
iris_label = iris.target
print('iris targetê°’: ', iris_label)
print('iris targetëª…: ', iris.target_names)

# ë¶ˆê½ƒ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìì„¸íˆ ë³´ê¸° ìœ„í•´ DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
iris_df = pd.DataFrame(data = iris_data, columns = iris.feature_names)
iris_df.head(3)

X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size = 0.2, random_state=11)

>>>
iris targetê°’:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
iris targetëª…:  ['setosa' 'versicolor' 'virginica']
sepal length (cm)	sepal width (cm)	petal length (cm)	petal width (cm)
0	5.1	3.5	1.4	0.2
1	4.9	3.0	1.4	0.2
2	4.7	3.2	1.3	0.2
```

- ğŸš¦X_train: í•™ìŠµìš© í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸
- ğŸš¦X_test: í…ŒìŠ¤íŠ¸ìš© í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸
- ğŸš¦y_train: í•™ìŠµìš© ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸ = íƒ€ê²Ÿ ë°ì´í„°
- ğŸš¦y_test: í…ŒìŠ¤íŠ¸ìš© ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸ = íƒ€ê²Ÿ ë°ì´í„°

### iris ë°ì´í„°ì…‹ ì¢€ ë” ì•Œì•„ë³´ê¸°

```python
from sklearn.datasets import load_iris

iris_data = load_iris()
print(type(iris_data))

>>>
<class 'sklearn.utils.Bunch'>
```

Bunch í´ë˜ìŠ¤ëŠ” íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ ìë£Œí˜•ê³¼ ìœ ì‚¬í•˜ë‹¤.
ê·¸ë˜ì„œ

```pyhton
keys = iris_data.keys()
print('ë¶ˆê½ƒ ë°ì´í„° ì„¸íŠ¸ì˜ í‚¤ë“¤: ', keys)

>>>
ë¶ˆê½ƒ ë°ì´í„° ì„¸íŠ¸ì˜ í‚¤ë“¤:  dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])
```

ì´ë ‡ê²Œ ë°ì´í„°ì˜ key ë“¤ë§Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë‹¤.

```python
print('\n feature_names ì˜ type: ', type(iris_data.feature_names))
print(' feature_namesì˜ shape:', len(iris_data.feature_names))
print(iris_data.feature_names)

print('\n target_namesì˜ type: ', type(iris_data.target_names))
print(' target_namesì˜ shape: ', len(iris_data.target_names))
print(iris_data.target_names)

print('\n dataì˜ type: ', type(iris_data.data))
print(' dataì˜ shape: ', iris_data.data.shape)
print(iris_data['data'][:10])

print('\n targetì˜ type: ', type(iris_data.target))
print(' targetì˜ shape: ', iris_data.target.shape)
print(iris_data.target[:10])

>>>
feature_names ì˜ type:  <class 'list'>
 feature_namesì˜ shape: 4
['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

 target_namesì˜ type:  <class 'numpy.ndarray'>
 target_namesì˜ shape:  3
['setosa' 'versicolor' 'virginica']

 dataì˜ type:  <class 'numpy.ndarray'>
 dataì˜ shape:  (150, 4)
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5.  3.6 1.4 0.2]
 [5.4 3.9 1.7 0.4]
 [4.6 3.4 1.4 0.3]
 [5.  3.4 1.5 0.2]
 [4.4 2.9 1.4 0.2]
 [4.9 3.1 1.5 0.1]]

 targetì˜ type:  <class 'numpy.ndarray'>
 targetì˜ shape:  (150,)
[0 0 0 0 0 0 0 0 0 0]
```

## 2ï¸âƒ£ í›ˆë ¨í•˜ê¸°

```python
# DecisionTreeClassifier ê°ì²´ ìƒì„±
dt_clf = DecisionTreeClassifier(random_state=11)
# í•™ìŠµ ìˆ˜í–‰
dt_clf.fit(X_train, y_train)
```

## 3ï¸âƒ£ ì˜ˆì¸¡í•˜ê¸°

```python
# í•™ìŠµì´ ì™„ë£Œëœ DecisionTreeClassifier ê°ì²´ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
pred = dt_clf.predict(X_test)

from sklearn.metrics import accuracy_score
print('ì˜ˆì¸¡ ì •í™•ë„: {0:.4f}'.format(accuracy_score(y_test, pred)))

>>>
ì˜ˆì¸¡ ì •í™•ë„: 0.9333
```

# Model Selection ëª¨ë“ˆ ì†Œê°œ

## train_test_split

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

dt_clf = DecisionTreeClassifier( )
iris_data = load_iris()

X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size = 0.3, random_state = 121)
dt_clf.fit(X_train, y_train)
pred = dt_clf.predict(X_test)
print('ì˜ˆì¸¡ ì •í™•ë„: {0:.4f}'.format(accuracy_score(y_test, pred)))
```

### train_test_split() íŒŒë¼ë¯¸í„°

- [í•„ìˆ˜] í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸: ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„°
- [í•„ìˆ˜] ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸: ë‘ ë²ˆì§¸ íŒŒë¼ë¯¸í„°
- [ì„ íƒ] test_size: ì „ì²´ ë°ì´í„°ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ í¬ê¸°ë¥¼ ì–¼ë§ˆë¡œ ìƒ˜í”Œë§ í•  ê²ƒì¸ê°€
- [ì„ íƒ] train_size: ì–˜ë³´ë‹¤ëŠ” test_sizeë¥¼ ë§ì´ ì‚¬ìš©
- [ì„ íƒ] shuffle: ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ê¸° ì „ì— ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì„ì„ì§€ë¥¼ ê²°ì •(default = True)
- [ì„ íƒ] random_State: ìˆ˜í–‰í•  ë•Œë§ˆë‹¤ ë™ì¼í•œ ë°ì´í„° ì„¸íŠ¸ë¡œ ë¶„ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©
- train_test_split(): í•™ìŠµìš© í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸, í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸, í•™ìŠµìš© ë°ì´í„° ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸, í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ì„¸íŠ¸ë¥¼ ë°˜í™˜

## KFold (a.k.a K í´ë“œ êµì°¨ ê²€ì¦)

í•™ìŠµìš© ë°ì´í„° ì„¸íŠ¸ë¥¼ n_splits ì˜ ê°’ë§Œí¼ ë‚˜ëˆ„ì–´ì„œ n_splits ë§Œí¼ í›ˆë ¨ì„ í•œë‹¤.

```python
# k-í´ë“œ êµì°¨ ê²€ì¦ í”„ë¡œì„¸ìŠ¤
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
import numpy as np

iris = load_iris()
features = iris.data
label = iris.target
dt_clf = DecisionTreeClassifier(random_state=156)

#5ê°œì˜ í´ë“œ ì„¸íŠ¸ë¡œ ë¶„ë¦¬í•˜ëŠ” KFold ê°ì²´ì™€ í´ë“œ ì„¸íŠ¸ë³„ ì •í™•ë„ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ê°ì²´ ìƒì„±
kfold = KFold(n_splits=5)
cv_accuracy = []
print('ë¶“ê½ƒ ë°ì´í„° ì„¸íŠ¸ í¬ê¸°: ', features.shape[0])
>>> ë¶“ê½ƒ ë°ì´í„° ì„¸íŠ¸ í¬ê¸°:  150

n_iter = 0

# kfold ê°ì²´ì˜ split()ì„ í˜¸ì¶œí•˜ë©´ í´ë“œë³„ í•™ìŠµìš©, ê²€ì¦ìš© í…ŒìŠ¤íŠ¸ì˜ ë¡œìš° ì¸ë±ìŠ¤ë¥¼ arrayë¡œ ë°˜í™˜
for train_index, test_index in kfold.split(features):
    # kfold.split()ìœ¼ë¡œ ë°˜í™˜ëœ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ í•™ìŠµìš©, ê²€ì¦ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]

    # í•™ìŠµ ë° ì˜ˆì¸¡
    dt_clf.fit(X_train, y_train)
    pred = dt_clf.predict(X_test)
    n_iter += 1

    # ë°˜ë³µì‹œë§ˆë‹¤ ì •í™•ë„ ì¸¡ì •
    accuracy = np.round(accuracy_score(y_test, pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print('\n{0} êµì°¨ ê²€ì¦ ì •í™•ë„: {1}, í•™ìŠµ ë°ì´í„° í¬ê¸°: {2}, ê²€ì¦ ë°ì´í„° í¬ê¸°: {3}'.format(n_iter, accuracy, train_size, test_size))
    print('#{0} ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤: {1}'.format(n_iter, test_index))
    cv_accuracy.append(accuracy)

# ê°œë³„ iteration ë³„ ì •í™•ë„ë¥¼ í•©í•˜ì—¬ í‰ê·  ì •í™•ë„ ê³„ì‚°
print('\n## í‰ê·  ê²€ì¦ ì •í™•ë„:', np.mean(cv_accuracy))

>>>
1 êµì°¨ ê²€ì¦ ì •í™•ë„: 1.0, í•™ìŠµ ë°ì´í„° í¬ê¸°: 120, ê²€ì¦ ë°ì´í„° í¬ê¸°: 30
1 ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]

2 êµì°¨ ê²€ì¦ ì •í™•ë„: 0.9667, í•™ìŠµ ë°ì´í„° í¬ê¸°: 120, ê²€ì¦ ë°ì´í„° í¬ê¸°: 30
2 ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤: [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53
 54 55 56 57 58 59]

3 êµì°¨ ê²€ì¦ ì •í™•ë„: 0.8667, í•™ìŠµ ë°ì´í„° í¬ê¸°: 120, ê²€ì¦ ë°ì´í„° í¬ê¸°: 30
3 ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤: [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83
 84 85 86 87 88 89]

4 êµì°¨ ê²€ì¦ ì •í™•ë„: 0.9333, í•™ìŠµ ë°ì´í„° í¬ê¸°: 120, ê²€ì¦ ë°ì´í„° í¬ê¸°: 30
4 ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤: [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119]

5 êµì°¨ ê²€ì¦ ì •í™•ë„: 0.7333, í•™ìŠµ ë°ì´í„° í¬ê¸°: 120, ê²€ì¦ ë°ì´í„° í¬ê¸°: 30
5 ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤: [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
 138 139 140 141 142 143 144 145 146 147 148 149]

 í‰ê·  ê²€ì¦ ì •í™•ë„: 0.9
```

## StratifiedKFold (a.k.a satratified K í´ë“œ)

StratifiedKFoldì˜ ê²½ìš°ëŠ” ì ë‹¹íˆ ë°ì´í„°ì˜ íƒ€ê¹ƒê°’ì„ ë¶„í¬ì‹œì¼œì¤„ ìˆ˜ ìˆë‹¤.

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=3)
n_iter = 0

for train_index, test_index in skf.split(iris_df, iris_df['label']):
    n_iter += 1
    label_train = iris_df['label'].iloc[train_index]
    label_test = iris_df['label'].iloc[test_index]
    print('## êµì°¨ ê²€ì¦: {0}'.format(n_iter))
    print('í•™ìŠµ ë ˆì´ë¸” ë°ì´í„° ë¶„í¬: \n', label_train.value_counts())
    print('ê²€ì¦ ë ˆì´ë¸” ë°ì´í„° ë¶„í¬: \n', label_test.value_counts())

>>>
## êµì°¨ ê²€ì¦: 1
í•™ìŠµ ë ˆì´ë¸” ë°ì´í„° ë¶„í¬:
 2    34
0    33
1    33
Name: label, dtype: int64
ê²€ì¦ ë ˆì´ë¸” ë°ì´í„° ë¶„í¬:
 0    17
1    17
2    16
Name: label, dtype: int64
## êµì°¨ ê²€ì¦: 2
í•™ìŠµ ë ˆì´ë¸” ë°ì´í„° ë¶„í¬:
 1    34
0    33
2    33
Name: label, dtype: int64
ê²€ì¦ ë ˆì´ë¸” ë°ì´í„° ë¶„í¬:
 0    17
2    17
1    16
Name: label, dtype: int64
## êµì°¨ ê²€ì¦: 3
í•™ìŠµ ë ˆì´ë¸” ë°ì´í„° ë¶„í¬:
 0    34
1    33
2    33
Name: label, dtype: int64
ê²€ì¦ ë ˆì´ë¸” ë°ì´í„° ë¶„í¬:
 1    17
2    17
0    16
Name: label, dtype: int64
```

stratiedKFoldì˜ ê²½ìš°ëŠ” í”¼ì²˜ì—ì„œ ì¼ë¶€, ë ˆì´ë¸”ì—ì„œ ì¼ë¶€ë¥¼ ë½‘ì•„ì˜¤ëŠ” ê²ƒì´ë¯€ë¡œ train_indexì™€ test_indexë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” í”¼ì²˜ì™€ ë ˆì´ë¸” ì „ì²´ì—ì„œ ëŒ€í•´ì„œ ì¸ë±ìŠ¤ë¥¼ ë½‘ì•„ì™€ì•¼ ëœë‹¤.

```python
dt_clf = DecisionTreeClassifier(random_state=156)

skfold = StratifiedKFold(n_splits=3)
n_iter = 0
cv_accuracy = []
# feature: iris.data
# label: iris.target
# StratifiedKFoldì˜ split() í˜¸ì¶œ ì‹œ ë°˜ë“œì‹œ ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸ë„ ì¶”ê°€ ì…ë ¥ í•„ìš”
for train_index, test_index in skfold.split(features, label):
    # split()ìœ¼ë¡œ ë°˜í™˜ëœ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ í•™ìŠµìš©, ê²€ì¦ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]
    # í•™ìŠµ ë° ì˜ˆì¸¡
    dt_clf.fit(X_train, y_train)
    pred = dt_clf.predict(X_test)

    # ë°˜ë³µ ì‹œë§ˆë‹¤ ì •í™•ë„ ì¸¡ì •
    n_iter += 1
    accuracy = np.round(accuracy_score(y_test, pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print('\n{0} êµì°¨ ê²€ì¦ ì •í™•ë„: {1}, í•™ìŠµ ë°ì´í„° í¬ê¸°: {2}, ê²€ì¦ ë°ì´í„° í¬ê¸°: {3}'.format(n_iter, accuracy, train_size, test_size))
    print('#{0} ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤: {1}'.format(n_iter, test_index))
    cv_accuracy.append(accuracy)
# êµì°¨ ê²€ì¦ë³„ ì •í™•ë„ ë° í‰ê·  ì •í™•ë„ ê³„ì‚°
print('\n## êµì°¨ ê²€ì¦ë³„ ì •í™•ë„:', np.round(cv_accuracy, 4))
print('## í‰ê·  ê²€ì¦ ì •í™•ë„:', np.round(np.mean(cv_accuracy), 4))

>>>
1 êµì°¨ ê²€ì¦ ì •í™•ë„: 0.98, í•™ìŠµ ë°ì´í„° í¬ê¸°: 100, ê²€ì¦ ë°ì´í„° í¬ê¸°: 50
1 ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  50
  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 100 101
 102 103 104 105 106 107 108 109 110 111 112 113 114 115]

2 êµì°¨ ê²€ì¦ ì •í™•ë„: 0.94, í•™ìŠµ ë°ì´í„° í¬ê¸°: 100, ê²€ì¦ ë°ì´í„° í¬ê¸°: 50
2 ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤: [ 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  67
  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82 116 117 118
 119 120 121 122 123 124 125 126 127 128 129 130 131 132]

3 êµì°¨ ê²€ì¦ ì •í™•ë„: 0.98, í•™ìŠµ ë°ì´í„° í¬ê¸°: 100, ê²€ì¦ ë°ì´í„° í¬ê¸°: 50
3 ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤: [ 34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  83  84
  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 133 134 135
 136 137 138 139 140 141 142 143 144 145 146 147 148 149]

 êµì°¨ ê²€ì¦ë³„ ì •í™•ë„: [0.98 0.94 0.98]
 í‰ê·  ê²€ì¦ ì •í™•ë„: 0.9667
```

stratiedKFoldì˜ ê²½ìš°ëŠ” ë¶„ë¥˜ì¼ ë•ŒëŠ” ì‚¬ìš©ì´ ë˜ì§€ë§Œ íšŒê·€ì—ì„œëŠ” StratifiedKFoldê°€ ì§€ì›ë˜ì§€ ì•ŠëŠ”ë‹¤.

## cross_val_score()

êµì°¨ ê²€ì¦ì„ ë³´ë‹¤ ê°„í¸í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤.

```
cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')
```

ì£¼ìš” íŒŒë¼ë¯¸í„°

- estimator: í•´ë‹¹ ëª¨ë¸ì´ ë¶„ë¥˜ì¸ì§€ íšŒê·€ì¸ì§€ë¥¼ ì˜ë¯¸
- X: í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸
- y: ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸
- scoring: ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€ ì§€í‘œë¥¼ ê¸°ìˆ (ì¼ë°˜ì ìœ¼ë¡œ accuracyë¥¼ ì‚¬ìš©)
- cv: êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜

ë¶„ë¥˜ê°€ ì…ë ¥ë˜ë©´ Stratified K í´ë“œ ë°©ì‹ìœ¼ë¡œ ë ˆì´ë¸”ê°’ì˜ ë¶„í¬ì— ë”°ë¼ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë¶„í• í•˜ì§€ë§Œ íšŒê·€ì¸ ê²½ìš°ëŠ” K í´ë“œ ë°©ì‹ìœ¼ë¡œ ë¶„í• í•œë‹¤.

```python
# êµì°¨ ê²€ì¦ì„ ë³´ë‹¤ ê°„í¸í•˜ê²Œ - cross_val_score()
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.datasets import load_iris

iris_data = load_iris()
dt_clf = DecisionTreeClassifier(random_state=156)

data = iris_data.data
label = iris_data.target

# ì„±ëŠ¥ ì§€í‘œëŠ” ì •í™•ë„(accuracy), êµì°¨ ê²€ì¦ ì„¸íŠ¸ëŠ” 3ê°œ
scores = cross_val_score(dt_clf, data, label, scoring='accuracy', cv=3)
print('êµì°¨ ê²€ì¦ë³„ ì •í™•ë„:', np.round(scores, 4))
print('í‰ê·  ê²€ì¦ ì •í™•ë„:', np.round(np.mean(scores), 4))

>>>
êµì°¨ ê²€ì¦ë³„ ì •í™•ë„: [0.98 0.94 0.98]
í‰ê·  ê²€ì¦ ì •í™•ë„: 0.9667
```

## GridSearchCV

êµì°¨ ê²€ì¦ê³¼ ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ í•œ ë²ˆì— í•  ìˆ˜ ìˆëŠ” í´ë˜ìŠ¤
cross-validationì„ ìœ„í•œ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ìë™ìœ¼ë¡œ ë¶„í• í•œ ë’¤ì— í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œì— ê¸°ìˆ ëœ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•´
ì‚¬ìš© ë°©ë²•ìœ¼ë¡œ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
import pandas as pd

# ë°ì´í„°ë¥¼ ë¡œë”©í•˜ê³  í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ë¦¬
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size = 0.3, random_state=121)

dtree = DecisionTreeClassifier()

### íŒŒë¼ë¯¸í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì„¤ì •
parameters = {'max_depth':[1, 2, 3], 'min_samples_split':[2, 3]}

# param_gridì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ 3ê°œì˜ train, test set foldë¡œ ë‚˜ëˆ„ì–´ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ì„¤ì •
### refit=Trueê°€ defaultì„. Trueì´ë©´ ê°€ì¥ ì¢‹ì€ íŒŒë¼ë¯¸í„° ì„¤ì •ìœ¼ë¡œ ì¬í•™ìŠµì‹œí‚´
grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)

# ë¶ˆê½ƒ í•™ìŠµ ë°ì´í„°ë¡œ param_gridì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ/í‰ê°€
grid_dtree.fit(X_train, y_train)

# GridSearchCV ê²°ê³¼ë¥¼ ì¶”ì¶œí•´ DataFrameìœ¼ë¡œ ë³€í™˜
scores_df = pd.DataFrame(grid_dtree.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score',
          'split0_test_score', 'split1_test_score', 'split2_test_score']]

>>>
params	mean_test_score	rank_test_score	split0_test_score	split1_test_score	split2_test_score
0	{'max_depth': 1, 'min_samples_split': 2}	0.657143	5	0.657143	0.657143	0.657143
1	{'max_depth': 1, 'min_samples_split': 3}	0.657143	5	0.657143	0.657143	0.657143
2	{'max_depth': 2, 'min_samples_split': 2}	0.933333	3	0.942857	0.914286	0.942857
3	{'max_depth': 2, 'min_samples_split': 3}	0.933333	3	0.942857	0.914286	0.942857
4	{'max_depth': 3, 'min_samples_split': 2}	0.942857	1	0.971429	0.914286	0.942857
5	{'max_depth': 3, 'min_samples_split': 3}	0.942857	1	0.971429	0.914286	0.942857
```

refit=Trueë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— ë”°ë¡œ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ë‹¤ì‹œ í›ˆë ¨ì‹œí‚¬ í•„ìš” ì—†ì´ ë°”ë¡œ ì •í™•ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
print('GridSearchCV ìµœì  íŒŒë¼ë¯¸í„°:', grid_dtree.best_params_)
print('GridSearchCV ìµœê³  ì •í™•ë„: {0:.4f}'.format(grid_dtree.best_score_))

>>>
GridSearchCV ìµœì  íŒŒë¼ë¯¸í„°: {'max_depth': 3, 'min_samples_split': 2}
GridSearchCV ìµœê³  ì •í™•ë„: 0.9429

# GridSearchCVì˜ refitìœ¼ë¡œ ì´ë¯¸ í•™ìŠµëœ estimator ë°˜í™˜
estimator = grid_dtree.best_estimator_

# GridSearchCVì˜ best_estimator_ëŠ” ì´ë¯¸ ìµœì  í•™ìŠµì´ ëìœ¼ë¯€ë¡œ ë³„ë„ í•™ìŠµì´ í•„ìš” ì—†ìŒ
pred = estimator.predict(X_test)
print('í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ì •í™•ë„: {0:.4f}'.format(accuracy_score(y_test, pred)))

>>>
í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ì •í™•ë„: 0.9556
```
