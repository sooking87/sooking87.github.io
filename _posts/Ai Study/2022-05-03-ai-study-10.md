---
title: "[chap 5-2] κµμ°¨ κ²€μ¦κ³Ό κ·Έλ¦¬λ“ μ„μΉ"
excerpt: "[chap 5-2] κµμ°¨ κ²€μ¦κ³Ό κ·Έλ¦¬λ“ μ„μΉ"
categories: [Ai Study]
tags: [Ai Study, Python]
toc: true
toc_sticky: true
---

## β— λ¬Έμ μ  β—

chap 5-1μ—μ„ max_depthλ¥Ό 3μΌλ΅ ν•κ³  λλ‚΄λ²„λ Έλ‹¤. ν•μ§€λ§ μµμ μ max_depthλ¥Ό μ°Ύμ§€λ” λ»ν–λ‹¤.

## π”® sol 1. κ²€μ¦ μ„ΈνΈ

κΈ°μ΅΄μ—λ” λ°μ΄ν„°λ“¤μ„ 80%/20%μ λΉ„μ¨λ΅ ν›λ ¨ μ„ΈνΈμ™€ λ°μ΄ν„° μ„ΈνΈλ΅ λ‚λ„μ—λ‹¤. μ΄λ΄ κ²½μ° κ²€μ¦μ„ λ»ν•λ‹¤. κ²€μ¦μ„ ν•κΈ° μ„ν•΄μ„λ” μ—¬λ¬ λ²μ ν›λ ¨μ„ ν•λ©΄ λμ§€λ§, κ°™μ€ λ°μ΄ν„°λ΅ μ—¬λ¬λ² ν›λ ¨μ„ ν•λ©΄ μ•λλ‹¤. λ”°λΌμ„ μ΄ λ¬Έμ λ¥Ό **_κ²€μ¦ μ„ΈνΈ_** λ¥Ό ν†µν•΄μ„ ν•΄κ²°ν–λ‹¤. κ²€μ¦ μ„ΈνΈλ” κΈ°μ΅΄μ λ°μ΄ν„°λ¥Ό 60%/20%/20% λΉ„μ¨λ΅ λ‚λ„μ–΄μ„ ν›λ ¨ μ„ΈνΈ, κ²€μ¦ μ„ΈνΈ, ν…μ¤νΈ μ„ΈνΈλ΅ λ‚λ„λ” κ²ƒμ΄λ‹¤.

```python
import pandas as pd
# λ°μ΄ν„° λ¶λ¬μ¤κ³ 
wine = pd.read_csv('https://bit.ly/wine_csv_data')

# νƒ€κΉƒμ΄λ‘ λ‚λ¨Έμ§€λ¥Ό λ¶„λ¦¬ν•΄μ„ λ„νμ΄ λ°°μ—΄λ΅ λ§λ“¤κ³ 
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

# νΈλ¦¬λ” λ”±ν μ „μ²λ¦¬ κ³Όμ •μ΄ ν•„μ”μ—†μΌλ‹κΉ pass
# ν›λ ¨ μ„ΈνΈ & ν…μ¤νΈ μ„ΈνΈ λ¶„λ¦¬

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
sub_input, val_input, sub_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)
print(sub_input.shape, val_input.shape)

(4157, 3) (1040, 3)
```

- sub_input, sub_target : 60%μ ν›λ ¨ μ„ΈνΈ
- val_input, val_target : 20%μ κ²€μ¦ μ„ΈνΈ
- test_input, test_target : 20%μ ν…μ¤νΈ μ„ΈνΈ

### π“ μ μ ν™•μΈ

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(sub_input, sub_target)
print(dt.score(sub_input, sub_target))
print(dt.score(val_input, val_target))

>>>
0.9971133028626413 # ν›λ ¨ μ„ΈνΈ μ μ
0.864423076923077 # κ²€μ¦ μ„ΈνΈ μ μ
```

## π”® sol 2. κµμ°¨ κ²€μ¦

κ²€μ¦ μ„ΈνΈλ¥Ό ν†µν•΄μ„ ν›λ ¨μ‹ν‚¬ κ²½μ° κ²€μ¦ μ„ΈνΈλ¥Ό λ§λ“λλΌ ν›λ ¨ μ„ΈνΈκ°€ μ¤„μ—λ‹¤. μ΄ λ¬Έμ λ¥Ό ν•΄κ²°ν•κΈ° μ„ν•΄μ„ **_κµμ°¨ κ²€μ¦_** μ„ μ‚¬μ©ν•λ‹¤. κµμ°¨ κ²€μ¦μ΄λ€ κ²€μ¦ μ„ΈνΈλ¥Ό λ–Όμ–΄ λ‚΄μ–΄ ν‰κ°€ν•λ” κ³Όμ •μ„ μ—¬λ¬ λ² λ°λ³µν•λ” κ²ƒμ΄λ‹¤. μ—¬κΈ°μ„ 80%μ λ°μ΄ν„°λ¥Ό kκ°λ΅ λ‚λ„μ–΄μ„ κ²€μ¦ μ„ΈνΈλ¥Ό λ½‘μ•„λ‚΄λ” κ³Όμ •μ„ _k-ν΄λ“ κµμ°¨ κ²€μ¦_ μ΄λΌκ³  ν•λ‹¤.
<br>
μ‚¬μ΄ν‚· λ°μ—λ” cross_validate() λΌλ” κµμ°¨ κ²€μ¦ ν•¨μκ°€ μλ‹¤.

```python
from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target)
print(scores)

>>> μ¶λ ¥κ°’
{'fit_time': array([0.0117085 , 0.01046944, 0.01088619, 0.01039219, 0.01086831]),
'score_time': array([0.0014801 , 0.00232983, 0.00132895, 0.00125623, 0.00124264]),
'test_score': array([0.86923077, 0.84615385, 0.87680462, 0.84889317, 0.83541867])}
```

- fit_time : λ¨λΈμ„ ν›λ ¨ν•λ” μ‹κ°„
- score_time : κ²€μ¦ν•λ” μ‹κ°„
- test_score : κ²€μ¦ ν΄λ“μ μ μ
  <br> <br>

cross_validate() ν•¨μλ” κΈ°λ³Έμ μΌλ΅ 5-ν΄λ“ κµμ°¨ κ²€μ¦μ„ μν–‰ν•κ³ , cv λ§¤κ°λ³€μλ¥Ό ν†µν•΄μ„ ν΄λ“ μλ¥Ό μμ •ν•  μ μλ‹¤.

```python
import numpy as np
print(np.mean(scores['test_score'])) # test_score ν‚¤μ— λ‹΄κΈ΄ 5κ°μ μ μμ ν‰κ· 

>>> 0.855300214703487
```

μ§€κΈ λ°μ΄ν„°λ“¤μ λ¶„λ¦¬ κ³Όμ •μ—μ„ train_test_split() ν•¨μλ¥Ό ν†µν•΄μ„ λ¶„λ¦¬ν•μ€μΌλ―€λ΅ λ”°λ΅ μ„μ„ ν•„μ”κ°€ μ—†λ‹¤. μ΄ κ²½μ°λ” λ”± 80%/20% μ¤‘μ—μ„ 20%λ” ν›λ ¨ μ„ΈνΈλ΅ λ–Όμ–΄ λ†“κ³  κµμ°¨ κ²€μ¦μ„ ν• κ²½μ°! ν•μ§€λ§ λ§μ•½ κµμ°¨ κ²€μ¦μ„ ν•  λ• ν›λ ¨ μ„ΈνΈλ¥Ό μ„μΌλ ¤λ¨Ό λ¶„ν• κΈ°λ¥Ό μ§€μ •ν•΄μ•Ό ν•λ‹¤.

### π“ ν΄λ“ μμ •ν•λ” λ°©λ²•

νκ·€ λ¨λΈμΌ κ²½μ° KFold λ¶„ν• κΈ°λ¥Ό μ‚¬μ©ν•κ³ , λ¶„λ¥ λ¨λΈμΌ κ²½μ° νƒ€κΉƒ ν΄λμ¤λ¥Ό κ³¨κ³ λ£¨ λ‚λ„κΈ° μ„ν•΄ StratifiedKFold λ¥Ό μ‚¬μ©ν•λ‹¤.

```python
from sklearn.model_selection import StratifiedKFold
scores = cross_validate(dt, train_input, train_target, cv = StratifiedKFold())
print(np.mean(scores['test_score']))

>>> 0.855300214703487
```

10-ν΄λ“ κµμ°¨ κ²€μ¦μ„ μν–‰ν•λ ¤λ©΄?

```python
splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_validate(dt, train_input, train_target, cv=splitter)
print(np.mean(scores['test_score']))

>>> 0.8574181117533719
```

## π”® κ·Έλ¦¬λ“ μ„μΉ

κ²€μ¦ μ„ΈνΈ, κµμ°¨ κ²€μ¦μ„ ν†µν•΄μ„ λ³΄λ‹¤ ν™•μ‹¤ν•κ² λ¨λΈμ„ κ²€μ¦ν•λ” λ°©λ²•μ— λ€ν•΄μ„ λ°°μ› λ‹¤. μ΄λ² μ λ¶€ν„°λ” μ΄λ¥Ό ν™μ©ν•μ—¬ μµμ μ max_depthλ¥Ό κµ¬ν•λ” λ°©λ²•μ„ μ•μ•„λ³΄μ.
<br>

- λ¨λΈ νλΌλ―Έν„° : λ¨Έμ‹ λ¬λ‹ λ¨λΈμ΄ ν•™μµν•λ” νλΌλ―Έν„°
- ν•μ΄νΌνλΌλ―Έν„° : λ¨λΈμ΄ ν•™μµν•  μ μ—†μ–΄μ„ μ‚¬μ©μκ°€ μ§€μ •ν•΄μ•Όλ§ ν•λ” νλΌλ―Έν„° like κµμ°¨ κ²€μ¦

<br>

min\*samples*splitμ΄ λ°”λ€λ©΄ μµμ μ max_depthλ„ λ°”λ€λ‹¤. κ·Έλ ‡λ‹¤λ©΄ μ§€κΈ λ§¤κ° λ³€μκ°€ 2κ°λΌλ” λ§μΈλ°, μ§€κΈμ€ λΉ„λ΅ 2κ°μ§€λ§ λ” λ§μ•„μ§€λ©΄ λ¬Έμ κ°€ λ³µμ΅ν•΄μ§„λ‹¤. -> sol. \*\*\_GridSearch*\*\*

### π“ λ§¤κ°λ³€μ μ§μ ‘ μ§€μ •

```python
from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease':[0.0001, 0.0002, 0.0003, 0.0004, 0.0005]} # κ°’μ„ λ°”κΏ”κ°€λ©΄μ„ μµμ μ min_impurity_decrease κ²°μ •
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1) # GridSearchCVμ cvκΈ°λ³Έκ°’μ€ 5.

gs.fit(train_input, train_target)
dt = gs.best_estimator_
print(dt.score(train_input, train_target)) # μ„μΉλ΅ μ°Ύμ€ μµμ μ λ§¤κ°λ³€μλ” best_params_ μ†μ„±μ— μ €μ¥!

print(gs.best_params_)
# {'min_impurity_decrease': 0.0001}

print(gs.cv_results_['mean_test_score'])
# [0.86819297 0.86453617 0.86492226 0.86780891 0.86761605]

best_index = np.argmax(gs.cv_results_['mean_test_score'])
print(gs.cv_results_['params'][best_index])
# {'min_impurity_decrease': 0.0001}
```

- min_impurity_decrease : μ–μ μµμ κ°’μ„ μ°Ύλ” κ²ƒμ΄ λ©ν‘
- GridSearchCV ν΄λμ¤ : νƒμƒ‰ λ€μƒ λ¨λΈ, params λ³€μ μ „λ‹¬μ„ ν†µν•΄μ„ κ·Έλ¦¬λ“ μ„μΉ κ°μ²΄λ¥Ό λ§λ“¬
- best*estimator* : β“
- best_params : κ·Έλ¦¬λ“ μ„μΉλ΅ μ°Ύμ€ μµμ μ λ§¤κ°λ³€μ
- cv*results*['mean_test_score'] : min_impurity_decreaseμ κ°’λ§λ‹¤ 5-ν΄λ“ κµμ°¨ κ²€μ¦μ„ μν–‰ν•μ—¬ λ‚μ¨ μ μμ ν‰κ· 

### π— μµμ μ max_depth κµ¬ν•κΈ°

```python
params = {'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),
          'max_depth': range(5, 20, 1),
          'min_samples_split': range(2, 100, 10)
          }
gs = GridSearchCV(DecisionTreeClassifier(random_state=42), params, n_jobs=-1)
gs.fit(train_input, train_target)
print(gs.best_params_)
# {'max_depth': 14, 'min_impurity_decrease': 0.0004, 'min_samples_split': 12}

print(np.max(gs.cv_results_['mean_test_score'])) # 0.8683865773302731
```

## π”® λλ¤ μ„μΉ

λ§¤κ°λ³€μμ κ°’μ΄ μμΉμΌ λ• κ°’μ λ²”μ„λ‚ κ°„κ²©μ„ λ―Έλ¦¬ μ •ν•κΈ° μ–΄λ ¤μΈ κ²½μ°κ°€ μλ‹¤. μ΄λ΄ λ• λλ¤ μ„μΉλ¥Ό μ‚¬μ©ν•λ‹¤.

```python
from scipy.stats import uniform, randint # uniform, randint ν΄λμ¤λ” λ¨λ‘ μ£Όμ–΄μ§„ λ²”μ„μ—μ„ κ³ λ¥΄κ² λ½‘λ”λ‹¤.
rgen = randint(0, 10)
rgen.rvs(10)
# array([9, 2, 0, 8, 1, 2, 6, 6, 4, 0])

np.unique(rgen.rvs(1000), return_counts=True)
# (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 99, 119,  98,  83, 104, 104,  99, 102,  94,  98]))

ugen = uniform(0, 1)
ugen.rvs(10)
# array([0.66538712, 0.75439689, 0.09605709, 0.36154309, 0.78712722, 0.54217307, 0.81125253, 0.90118702, 0.1911896 , 0.4142776 ])
```

- randint : λλ¤μΌλ΅ μ •μκ°’ λ½‘κΈ°
- uniform : λλ¤μΌλ΅ μ‹¤μ«κ°’ λ½‘κΈ°
  <br> <br>

- `rgen = randint(0, 10)` : 0λ¶€ν„° 10κΉμ§€μ λ²”μ„λ¥Ό κ°–λ” κ°μ²΄ μƒμ„±
- `rgen.rvs(10)` : rgen κ°μ²΄μ—μ„ 10κ°λ¥Ό λ½‘κΈ°
- `np.unique(rgen.rvs(1000), return_counts=True)` : 1000κ° λ½‘μ•„μ„ ν•΄λ‹Ή μ«μμ™€ κ°μ λ¦¬ν„΄
  <br> <br>

- `ugen = uniform(0, 1)` : rgenκ³Ό λ™μΌν•κ² 0λ¶€ν„° 1κΉμ§€μ λ²”μ„λ¥Ό κ°–λ” κ°μ²΄ μƒμ„±
- `ugen.rvs(10)` : 10κ° λ½‘κΈ°

```python
from sklearn.model_selection import RandomizedSearchCV

gs = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), params, n_iter=100, n_jobs=-1, random_state=42)

gs.fit(train_input, train_target)
print(gs.best_params_)
print(np.max(gs.cv_results_['mean_test_score']))

dt = gs.best_estimator_
print(dt.score(test_input, test_target))

>>>
{'max_depth': 39, 'min_impurity_decrease': 0.00034102546602601173, 'min_samples_leaf': 7, 'min_samples_split': 13}
0.8695428296438884 # ν›λ ¨ μ„ΈνΈμ μµκ³  μ μ
0.86 # ν…μ¤νΈ μ„ΈνΈ μ μ
```
