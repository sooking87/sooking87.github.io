---
title: "[chap 5-1] κ²°μ • νΈλ¦¬"
excerpt: "[chap 5-1] κ²°μ • νΈλ¦¬"
categories: [Ai Study]
tags: [Ai Study, Python]
toc: true
toc_sticky: true
---

## β— MISSION β—

ν™”μ΄νΈ μ™€μΈμΈμ§€ λ λ“ μ™€μΈμΈμ§€ κµ¬λ³„ν•κΈ° -> HOW?

## π’΅ sol 1. λ΅μ§€μ¤ν‹± νκ·€

### 1οΈβƒ£ λ°μ΄ν„° μ¤€λΉ„ν•κΈ°

νƒ€κ²κ°’μ΄ ν™”μ΄νΈ μ™€μΈ(1)μΈμ§€, λ λ“ μ™€μΈ(0)μΈμ§€λ¥Ό κµ¬ν•΄λλ” μƒν™©μ΄λ‹¤. λ‘ μ¤‘μ— ν•λ‚λ¥Ό νƒ€κ²μΌλ΅ μ •ν•λ” κ²ƒμ΄λ―€λ΅ λ¶„λ¥ λ¨λΈμ„ μ‚¬μ©ν•μ€λ‹¤. κ·Έ μ¤‘μ—μ„ **_λ΅μ§€μ¤ν‹± νκ·€_** λ¬Έμ λ¥Ό μ‚¬μ©! λ΅μ§€μ¤ν‹° νκ·€λ΅ μ΄μ§„ λ¶„λ¥ν•κΈ°

```python
import pandas as pd
wine = pd.read_csv('https://bit.ly/wine_csv_data')
wine.head() # νλ‹¤μ¤ λ°μ΄ν„°ν”„λ μ„μΌλ΅ μ λ€λ΅ μ½μ—λ”μ§€ μ²μ 5κ° ν™•μΈ

wine.info()
wine.describe()

# λ„νμ΄ λ°°μ—΄λ΅ λ°”κΎΈκΈ°
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

# ν›λ ¨ μ„ΈνΈ, ν…μ¤νΈ μ„ΈνΈ λ¶„λ¦¬
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size = 0.2, random_state=42)
print(train_input.shape, test_input.shape) # ν›λ ¨ μ„ΈνΈ (5197, 3), ν…μ¤νΈ μ„ΈνΈ (1300, 3)

# ν‘μ¤€ν™”
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

>>> μ¶λ ¥κ°’
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 6497 entries, 0 to 6496
Data columns (total 4 columns):
 #   Column   Non-Null Count  Dtype
---  ------   --------------  -----
 0   alcohol  6497 non-null   float64
 1   sugar    6497 non-null   float64
 2   pH       6497 non-null   float64
 3   class    6497 non-null   float64

dtypes: float64(4)
memory usage: 203.2 KB
alcohol	sugar	pH	class
count	6497.000000	6497.000000	6497.000000	6497.000000
mean	10.491801	5.443235	3.218501	0.753886
std	1.192712	4.757804	0.160787	0.430779
min	8.000000	0.600000	2.720000	0.000000
25%	9.500000	1.800000	3.110000	1.000000
50%	10.300000	3.000000	3.210000	1.000000
75%	11.300000	8.100000	3.320000	1.000000
max	14.900000	65.800000	4.010000	1.000000
```

- νλ‹¤μ¤ λ°μ΄ν„°ν”„λ μ„μ— μ μ©ν• λ©”μ†λ“
  - info() : κ° μ—΄μ λ°μ΄ν„° νƒ€μ…κ³Ό λ„λ½λ λ°μ΄ν„°κ°€ μλ”μ§€ ν™•μΈμ— μ μ©
  - describe() : μ—΄μ— λ€ν• κ°„λµν• ν†µκ³„λ¥Ό μ¶λ ¥, μ„ κ²°κ³Όκ°’μ„ λ³΄κ³  μ¤μΌ€μΌμ΄ λ‹¤λ¦„μ„ ν™•μΈν•  μ μλ‹¤. <br>

κ·Έλ¦¬κ³ ! ν›λ ¨μ„ ν•κΈ° μ„ν•΄μ„ λ°μ΄ν„°λ¥Ό μ¤€λΉ„ν•λ” κ³Όμ •μ€ ν•­μƒ μ μ‚¬ν•λ‹¤.

1. λ°μ΄ν„° λ¶λ¬μ¤κΈ°(check: `wine.head()`)
2. λ„νμ΄ λ°°μ—΄λ΅ λ°”κΎΈκΈ° / μ΄λ•, νƒ€κΉƒκ°’κ³Ό κ·Έ μ™Έμ ν΄λμ¤λ¥Ό λ¶„λ¦¬ν•μ—¬ λ„νμ΄ λ°°μ—΄λ΅ λ¶„λ¦¬μ‹ν‚µλ‹λ‹¤ (`~~.to_numpy()`)
3. ν›λ ¨ μ„ΈνΈμ™€ ν…μ¤νΈ μ„ΈνΈλ΅ λ¶„λ¦¬(`from sklearn.model_selection import train_test_split`)
4. (ν•„μ”ν•λ‹¤λ©΄) ν‘μ¤€ν™”(`from sklearn.preprocessing import StandardScaler`)

### 2οΈβƒ£ λ΅μ§€μ¤ν‹± νκ·€ λ¨λΈ μ‚¬μ©

```python
# λ΅μ§€μ¤ν‹± νκ·€ λ¨λΈ μ‚¬μ©
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
print(lr.coef_, lr.intercept_)

>>> μ¶λ ¥κ°’
0.7808350971714451
0.7776923076923077
[[ 0.51270274  1.6733911  -0.68767781]] [1.81777902]
```

β— λ¬Έμ μ  β— <br>
μΌλ‹¨μ€ μ μκ°€ μ „μ²΄μ μΌλ΅ λ†’μ§€ μ•μΌλ―€λ΅ κ³Όμ†μ ν•©μ΄ λμ—λ‹¤. <br>

π’΅ ν•΄κ²°μ±… π’΅ <br>

1. κ·μ  λ§¤κ°λ³€μ κ°’μ„ λ°”κΎΌλ‹¤.
2. solver λ§¤κ°λ³€μλ¥Ό ν†µν•΄μ„ μ„ ν• λ¨λΈμ΄ μ•„λ‹ λ‹¤λ¥Έ μ•κ³ λ¦¬μ¦μ„ μ„ νƒν•λ‹¤.
3. κ²°μ • νΈλ¦¬

## π”® κ²°μ • νΈλ¦¬

κ²°μ • νΈλ¦¬ λ¨λ“¤μ€ μ΄μ λ¥Ό μ„¤λ…ν•κΈ° μ‰½λ‹¤. -> μ™¤κΉ?

### π“ μ μ ν™•μΈ

```python
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target)) # ν›λ ¨ μ„ΈνΈ
print(dt.score(test_scaled, test_target)) # ν…μ¤νΈ μ„ΈνΈ

>>> μ¶λ ¥κ°’
0.996921300750433
0.8592307692307692
```

ν›λ ¨ μ„ΈνΈ μ μκ°€ ν…μ¤νΈ μ„ΈνΈ μ μλ³΄λ‹¤ λ†’κ² λ‚μ™€μ„ ν„μ¬λ” κ³Όλ€μ ν•©λ λ¨λΈμ΄λΌκ³  ν•  μ μλ‹¤. κ·Όλ° κ²°μ • νΈλ¦¬ ν΄λμ¤ `DecisionTreeClassifier` λ” plot_tree() ν•¨μλ¥Ό μ‚¬μ©ν•΄ κ²°μ • νΈλ¦¬λ¥Ό μ΄ν•΄ν•κΈ° μ‰¬μ΄ κ·Έλ¦ΌμΌλ΅ μ¶λ ¥ν•΄μ¤„ μ μλ‹¤. νΈλ¦¬μ κ²°μ • κµ¬μ΅°λ¥Ό ν™•μΈν• ν›„, κ³Όλ€μ ν•©μ„ μ¤„μ—¬λ³΄λ„λ΅ ν•μ!

```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
plt.figure(figsize=(10, 7))
plot_tree(dt)
plt.show()
```

![download1](https://user-images.githubusercontent.com/96654391/166252140-25eb3680-4477-4515-92ec-63f2f3838c32.png) <br>
κ·Έλ¦Ό λ°°κ²½μ΄ ν¬λ…μ΄λΌ μ μ•λ³΄μΌ κ²ƒ κ°™κΈ΄ ν•μ§€λ§,,, νΈλ¦¬λ¥Ό ν†µν•΄μ„ ν›λ ¨λ νΈλ¦¬μ λ¨μµμ„ λ³Ό μ μλ‹¤. π³ λ΅ ν‘μ‹λ ν•΄λ”λ” **_plot_tree()_** ν•¨μμ λ§¤κ°λ³€μ λλ” κ΄€λ ¨ λ‚΄μ©μ΄λ‹¤.

### π³ max_depth λ§¤κ°λ³€μ

μ„ νΈλ¦¬ κ·Έλ¦Όμ„ λ³΄λ©΄ λ„λ¬΄ μ‘μ•„μ„ λ…Έλ“ μ•μ κΈ°μ¤€μ μ΄ ν•λ‚λ„ λ³΄μ΄μ§€ μ•λ”λ‹¤. μ΄λ¥Ό λ³Ό μ μκ², μ¶λ ¥λλ” λ…Έλ“μ κ°μλ¥Ό μ΅°μ •ν•λ” λ§¤κ°λ³€μκ°€ **max_depth** λ§¤κ°λ³€μμ΄λ‹¤.

```python
plt.figure(figsize=(10, 7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

![download2](https://user-images.githubusercontent.com/96654391/166252811-359b8bb3-f990-4298-8a3f-7aa55550a39d.png)
<br>

μ½”λ“λ¥Ό λ³΄λ©΄ `max_depth=1` μ΄λΌκ³  μ§€μ •μ„ ν•΄λ†“μ•λ”λ°, μ΄λ” λ£¨νΈ λ…Έλ“λ¥Ό μ μ™Έν• λ…Έλ“ 1κ°λ§ μ¶λ ¥ν•΄μ£Όλ” κ²ƒμ„ μ• μ μλ‹¤. λ…Έλ“ κ°€μ§€μ μ™Όμ½μ€ YES, μ¤λ¥Έμ½μ€ NOλ¥Ό μλ―Έν•λ©°, κΈ°μ¤€μ μ€ λ…Έλ“μ μ μΌ μ„μ— μ ν€μλ” input κ°’μ΄λ‹¤. giniλ” λ¶μλ„λ΅ λ‹¤μ μ μ—μ„ μ„¤λ…ν•  μμ •. samplesλ” μ΄ μƒν”μ μλ¥Ό μλ―Έν•κ³  valueλ” ν΄λμ¤λ³„ μƒν” μλ¥Ό μλ―Έν•λ‹¤.

### π³ filled λ§¤κ°λ³€μ

`filled = true` λ¥Ό μ§€μ •ν•λ©΄ ν΄λμ¤λ§λ‹¤ μƒ‰κΉ”μ„ λ¶€μ—¬ν•κ³ , μ–΄λ–¤ ν΄λμ¤μ λΉ„μ¨μ΄ λ†’μ•„μ§μλ΅ μ μ  μ§„ν•μƒ‰μ„ λ‚νƒ€λ‚Έλ‹¤.

### π³ gini λ§¤κ°λ³€μ

gini λ§¤κ°λ³€μλ” μ§€λ‹ λ¶μλ„λ¥Ό μλ―Έν•λ‹¤. DecisionTreeClassifier ν΄λμ¤μ criterion λ§¤κ°λ³€μμ κΈ°λ³Έκ°’μ΄ _gini_ μ΄λ‹¤. criterion λ§¤κ°λ³€μμ μ©λ„λ” λ…Έλ“μ—μ„ λ°μ΄ν„°λ¥Ό λ¶„ν• ν•  κΈ°μ¤€μ„ μ •ν•λ” κ²ƒμΌλ΅ λ£¨νΈ λ…Έλ“μ λ‹Ήλ„κ°€ μ™ ν•ν•„ -0.239λ¥Ό κΈ°μ¤€μΌλ΅ λ‚λ‰κ² λμ—λ”μ§€λ¥Ό μ• μ μλ‹¤.
<br>

> μ§€λ‹ λ¶μλ„ = 1 - (μμ„± ν΄λμ¤ λΉ„μ¨<sup>2</sup> + μ–‘μ„± ν΄λμ¤ λΉ„μ¨<sup>2</sup>) <br>

κ²°μ • νΈλ¦¬ λ¨λΈμ€ λ¶€λ¨ λ…Έλ“μ™€ μμ‹ λ…Έλ“μ λ¶μλ„ μ°¨μ΄κ°€ κ°€λ¥ν• ν¬λ„λ΅ μ„±μ¥μ‹ν‚¨λ‹¤. μ΄ λ¶μλ„ μ°¨μ΄λ¥Ό μ •λ³΄ μ΄λ“μ΄λΌκ³  ν•κ³ , **μ •λ³΄ μ΄λ“μ΄ μµλ€κ°€ λλ„λ΅ λ°μ΄ν„°λ¥Ό λ‚λ„λ” κ²ƒμ΄λ‹¤.** μ§€λ‹κ°€ μ κ³±μ„ ν†µν•΄μ„ λ¶μλ„λ¥Ό κ³„μ‚°ν–λ‹¤λ©΄ criterion='entropy'λΌλ” κ°’μ€ λ΅κ·Έλ¥Ό ν†µν•΄μ„ λ¶μλ„λ¥Ό κ²€μ‚¬ν•λ‹¤. ν•μ§€λ§ μ΄λ” λ¶€λ¨ λ…Έλ“μ™€ μμ‹ λ…Έλ“κ°„μ μ°¨μ΄κ°€ μ „μ²΄μ μΌλ΅ μ κ² λ‚λ―€λ΅ μ΄ μ±…μ—μ„λ” μ§€λ‹ λ¶μλ„λ¥Ό μ‚¬μ©ν•λ‹¤.

### π³ κ°€μ§€μΉκΈ°

μ•„κΉ κ³Όλ€μ ν•©μ λ¬Έμ κ°€ λ°μƒν–λ‹¤. μ΄ λ¬Έμ λ¥Ό ν•΄κ²°ν•κΈ° μ„ν•΄μ„λ” μ μ ν• κ°€μ§€μΉκΈ°κ°€ ν•„μ”ν•λ‹¤. κ°€μ§€μΉκΈ°λ” max_depthλ¥Ό ν†µν•΄μ„ μ΄λ£¨μ–΄μ§€κ³  max_depthλ¥Ό κ²°μ • μ§€μ€ λ‹¤μ μ΄ μ«μλ§νΌμ„ ν›λ ¨μ‹ν‚¤λ―€λ΅μ¨ κ°€μ§€μΉκΈ°λ¥Ό ν†µν•΄ ν›λ ¨ μ μλ¥Ό μ μ–΄ν•λ‹¤.

```python
# κ°€μ§€μΉκΈ°
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)
print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))

plt.figure(figsize=(20, 15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()

>>> μ¶λ ¥κ°’
0.8454877814123533
0.8415384615384616
```

![download3](https://user-images.githubusercontent.com/96654391/166255445-1d626590-2f23-4cf4-991c-bff5e3532c7a.png)
<br>

μ„ μ‚¬μ§„μ„ λ³΄λ©΄ νλ€ κ³„μ—΄ λ…Έλ“κ°€ μκ³  λΉ¨κ°„ κ³„μ—΄ λ…Έλ“κ°€ μλ‹¤. νλ€ κ³„μ—΄ λ…Έλ“λ” μ–‘μ„± ν΄λμ¤κ°€ λ§μ€ λΉ„μ¨μ„ μ°¨μ§€ν•λ” κ²½μ°μ΄κ³ , λΉ¨κ°„ λ…Έλ“λ” μμ„± ν΄λμ¤κ°€ λ§μ€ λΉ„μ¨μ„ μ°¨μ§€ν•λ” κ²½μ°μ΄λ‹¤. μμ„Έν λ³΄λ©΄ μΈν’‹κ°’λ„ λ…Έλ“λ“¤λ§λ‹¤ λ‹¤λ¥Έλ° μ΄λ¥Ό ν†µν•΄μ„ ν•΄λ‹Ή λ²”μ„μ— λ§λ” μƒν”λ“¤μ„ κ²°μ •ν•΄ λ‚μ•„κ°„λ‹¤. μ •λ§ μ¤‘μ”ν•μ μ€ κ²°μ • νΈλ¦¬ μ•κ³ λ¦¬μ¦μ€ μ¤μΌ€μΌμ΄ ν•„μ”μ—†μΌλ―€λ΅ **ν‘μ¤€ν™” μ „μ²λ¦¬ κ³Όμ •μ΄ ν•„μ” μ—†λ‹¤.**

## π— κ²°κ³Ό

```python
# νΈλ¦¬λ” ν‘μ¤€ν™” μ „μ²λ¦¬κ°€ ν•„μ” μ—†μ
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)
print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))

plt.figure(figsize=(20, 15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()

>>> μ¶λ ¥κ°’
0.8454877814123533
0.8415384615384616
```

μμ„Έν λ³΄λ©΄ μ „μ²λ¦¬λ λ°μ΄ν„°κ°€ μ•„λ‹λΌ μ „μ²λ¦¬ μ „ λ°μ΄ν„°λΌλ” μ ,,!
![download3](https://user-images.githubusercontent.com/96654391/166256498-56c82ffe-f609-4d5b-8fe9-190d3f5ab458.png)
