---
title: "[chap 5-3] νΈλ¦¬μ μ•™μƒλΈ”"
excerpt: "[chap 5-3] νΈλ¦¬μ μ•™μƒλΈ”"
categories: [Ai Study]
tags: [Ai Study, Python]
toc: true
toc_sticky: true
---

## π– μ •ν• λ°μ΄ν„°μ™€ λΉ„μ •ν• λ°μ΄ν„°

- μ •ν• λ°μ΄ν„° : μ–΄λ–¤ κµ¬μ΅°λ΅ λμ–΄ μλ” ν•νƒ, CSV, λ°μ΄ν„°λ² μ΄μ¤, μ—‘μ…€
- λΉ„μ •ν• λ°μ΄ν„° : λ°μ΄ν„°λ² μ΄μ¤λ‚ μ—‘μ…€λ΅ ν‘ν„ν•κΈ° μ–΄λ ¤μ΄ κ²ƒ, ν…μ¤νΈ λ°μ΄ν„°, μ‚¬μ§„, μμ•… λ“±
  <br>
  <br>

μ§€κΈκΉμ§€ λ°°μ΄ λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ€ μ •ν• λ°μ΄ν„°μ— μ λ§μ•λ‹¤. κ·Έμ¤‘μ— μ •ν• λ°μ΄ν„°λ¥Ό λ‹¤λ£¨λ” λ° κ°€μ¥ λ›°μ–΄λ‚ μ„±κ³Όλ¥Ό λ‚΄λ” μ•κ³ λ¦¬μ¦μ΄ **_μ•™μƒλΈ” ν•™μµ_** μ΄λ‹¤. μ•™μƒλΈ” ν•™μµ ν•κΈ° μ „μ— μΌλ‹¨ λ°μ΄ν„°λ¶€ν„° μ •λ¦¬ν•΄ λ†“μ!

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# 1. λ°μ΄ν„° λ¶λ¬μ™€
wine = pd.read_csv('https://bit.ly/wine_csv_data')

# 2. targetκ³Ό λ°μ΄ν„° λ¶„λ¦¬
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()

# νΈλ¦¬λ” μ „μ²λ¦¬ ν•„μ” μ—†μΌλ―€λ΅ pass
# 3. ν›λ ¨ μ„ΈνΈμ™€ ν…μ¤νΈ μ„ΈνΈ λ¶„λ¦¬
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
```

# β¨ μ•™μƒλΈ” ν•™μµ

## π”® λλ¤ ν¬λ μ¤νΈ

λλ¤ ν¬λ μ¤νΈλ” κ²°μ • νΈλ¦¬λ¥Ό λλ¤ν•κ² λ§λ“¤μ–΄ κ²°μ • νΈλ¦¬μ μ²μ„ λ§λ“ λ‹¤. κ° κ²°μ • νΈλ¦¬μ μμΈ΅μ„ μ‚¬μ©ν•΄ μµμΆ… μμΈ΅μ„ λ§λ“ λ‹¤.

### π“ ν•™μµ λ°©λ²•

1.  κ° νΈλ¦¬λ¥Ό ν›λ ¨ν•κΈ° μ„ν• λ°μ΄ν„°λ¥Ό λλ¤μΌλ΅ λ§λ“ λ‹¤. ν›λ ¨ λ°μ΄ν„°μ—μ„ λλ¤ν•κ² μƒν” μ¶”μ¶ -> ν›λ ¨ λ°μ΄ν„°λ¥Ό λ§λ“¬(μ¤‘λ³µ ν—μ©)
    - 1000κ°μ μƒν” μ¤‘ 100κ°μ μƒν”μ„ λ½‘λ”λ‹¤κ³  ν•  λ•, μ—¬κΈ°μ„ ν•λ²μ— 100κ°μ μƒν”μ„ λ½‘λ”κ² μ•„λ‹λΌ 1000κ° μ¤‘μ— ν• κ°λ¥Ό λ½‘κ³ , λ‹¤μ‹ λ„£κ³ , λ 1000κ° μ¤‘μ— ν• κ°λ½‘κ³ ,, μ΄λ° μ‹μΌλ΅ 100λ² μ§„ν–‰ = μ΄λ ‡κ² λ§λ“¤μ–΄μ§„ μƒν”μ„ **λ¶€νΈμ¤νΈλ© μƒν”** μ΄λΌκ³  λ¶€λ¥Έλ‹¤.
2.  μ΄λ ‡κ² λ§λ“¤μ–΄μ§„ λ¶€νΈμ¤νΈλ© μƒν”μ„ κ°€μ§€κ³  κ°κ°μ κ²°μ • νΈλ¦¬λ¥Ό ν›λ ¨ μ‹ν‚¨λ‹¤.
3.  λ¶„λ¥ κ³Όμ •μΌ λ•λ” κ° νΈλ¦¬μ ν΄λμ¤λ³„ ν™•λ¥ μ„ ν‰κ· ν•μ—¬ κ°€μ¥ λ†’μ€ ν™•λ¥ μ„ κ°€μ§„ ν΄λμ¤λ¥Ό μμΈ΅μΌλ΅ μ‚Όλ”λ‹¤. νκ·€μΌ λ•λ” κ° νΈλ¦¬μ μμΈ΅μ„ ν‰κ· ν•λ‹¤.

### π“ ν›λ ¨ μ„ΈνΈ μ μ, κ²€μ¦ μ„ΈνΈ μ μ κµ¬ν•κΈ°

```python
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>> 0.9973541965122431 # ν›λ ¨ μ„ΈνΈ μ μ
    0.8905151032797809 # κ²€μ¦ μ„ΈνΈ μ μ
```

- `RandomForestClassifier` : λλ¤ ν¬λ μ¤νΈ ν΄λμ¤μ— μ μ©ν•΄λ³΄λ” λ¶„λ¥ λ¬Έμ , κΈ°λ³Έμ μΌλ΅ 100κ°μ κ²°μ • νΈλ¦¬λ¥Ό μ‚¬μ©(default)
- rf : λλ¤ ν¬λ μ¤νΈ κ°μ²΄ μƒμ„±
  - n_jobs=-1 : λ¨λ“  CPU μ½”μ–΄ μ‚¬μ©ν•  κ²ƒ
- `cross_validate(estimator, X, Y=None, scoring=None, cv=None, n_jobs=None, vaerbose=0, )` : κµμ°¨ κ²€μ¦ μν–‰ -> β“ κµμ°¨ κ²€μ¦μ„ ν†µν•΄μ„ λ½‘μ€ ν›λ ¨ μ„ΈνΈμ™€ κ²€μ¦ μ„ΈνΈλ¥Ό 1κ°μ κ²°μ • νΈλ¦¬μ— ν›λ ¨μ„ μ‹ν‚¨λ‹¤λ” λ§μΈκ°€? γ…‡γ…‡γ…‡ μ΄κ±°λ¥Ό 100λ²ν•΄μ„ κ° νΈλ¦¬μ ν΄λμ¤λ³„ ν™•λ¥ μ„ κµ¬ν•΄μ„ μμΈ΅ν•λ‹¤λ” κ±°? γ…‡γ…‡γ…‡π’΅
  - estimator : ν‰κ°€ν•λ ¤λ” λ¨λΈ
  - x : ν›λ ¨ λ°μ΄ν„°
  - y : νƒ€κΉƒ
  - cv : κµμ°¨ κ²€μ¦ λ¶„ν•  μ
  - n_jobs : CPU μ½”μ–΄ μ‚¬μ© κ°μ
  - return_train_score : ν›λ ¨ μ μ ν¬ν•¨ μ—¬λ¶€(κ²€μ¦ μ μλΏλ§ μ•„λ‹λΌ ν›λ ¨ μ„ΈνΈμ μ μλ„ κ°™μ΄ λ°ν™)

### π“ νΉμ„± μ¤‘μ”λ„ & OOB μƒν”

- νΉμ„± μ¤‘μ”λ„

  ```python
  rf.fit(train_input, train_target)
  print(rf.feature_importances_)
  >>> 0.8934000384837406
  ```

  μ• μ¥μ—μ„ ν–λ κ²°μ • νΈλ¦¬ ν• κ°μ νΉμ„± μ¤‘μ”λ„μ™€ λλ¤ ν¬λ μ¤νΈ ν΄λμ¤λ¥Ό ν†µν• νΉμ„± μ¤‘μ”λ„λ” μ΅°κΈ μ°¨μ΄κ°€ μλ‹¤. λλ¤ ν¬λ μ¤νΈλ” μ• μ΄μ— μƒν”μ„ λ½‘μ„ λ•, λ¶€νΈμ¤νΈλ© μƒν”μ„ λ½‘μ•„μ„ ν•λ‚μ νΉμ„±μ— κ³Όλ„ν•κ² μ§‘μ¤‘ν•μ§€ μ•κ³  μΆ€ λ” λ§μ€ νΉμ„±μ΄ ν›λ ¨ν•  κΈ°νλ¥Ό μ¤€λ‹¤. = κ³Όλ€μ ν•©μ„ μ¤„μ΄κ³  μΌλ°ν™”ν•  μ μλ‹¤.

- OOB μƒν”

```python
rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
```

OOB μƒν”μ€ λ¶€νΈμ¤νΈλ© μƒν”μ— ν¬ν•¨λμ§€ μ•κ³  λ‚¨λ” μƒν”λ΅ κ²€μ¦ μ„ΈνΈμ μ—­ν• μ„ ν•λ‹¤.

## π”® μ—‘μ¤νΈλΌ νΈλ¦¬

λλ¤ ν¬λ μ¤νΈμ™€ λΉ„μ·ν•μ§€λ§ λ¶€νΈμ¤νΈλ© μƒν”μ„ μ‚¬μ©ν•μ§€ μ•λ”λ‹¤λ” μ°¨μ΄μ μ΄ μλ‹¤. μ¦‰, κ° κ²°μ • νΈλ¦¬λ¥Ό λ§λ“¤ λ–Ό, μ „μ²΄ ν›λ ¨ μ„ΈνΈλ¥Ό μ‚¬μ©ν•λ‹¤. λ€μ‹  λ¬΄μ‘μ„λ΅ λ…Έλ“ κ²°μ •!

### π“ ν›λ ¨ μ„ΈνΈ μ μ, κ²€μ¦ μ„ΈνΈ μ μ κµ¬ν•κΈ°

```python
from sklearn.ensemble import ExtraTreesClassifier
et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.9974503966084433 # ν›λ ¨ μ„ΈνΈ μ μ
0.8887848893166506 # κ²€μ¦ μ„ΈνΈ μ μ
```

νΉμ„±μ΄ λ§μ§€ μ•μ•„ λ‘ λ¨λΈμ μ°¨μ΄κ°€ ν¬μ§€ μ•μ§€λ§ λ³΄ν†µ μ—‘μ¤νΈλΌ νΈλ¦¬κ°€ λ¬΄μ‘μ„μ„±μ΄ μΆ€ λ” ν¬κΈ° λ•λ¬Έμ— λλ¤ ν¬λ μ¤νΈλ³΄λ‹¤ λ” λ§μ€ κ²°μ • νΈλ¦¬λ¥Ό ν›λ ¨ν•΄μ•Ό λλ‹¤. μ¥ : λλ¤ν•κ² λ…Έλ“λ¥Ό λ¶„ν•  -> κ³„μ‚° μ†λ„ λΉ λ¦„

### π“ νΉμ„± μ¤‘μ”λ„

```python
et.fit(train_input, train_target)
print(et.feature_impotances+)

>>> [0.20183568 0.52242907 0.27573525]
```

μ—‘μ¤νΈλΌ νΈλ¦¬μ νκ·€ λ²„μ „ : ExtraTreesRegressor ν΄λμ¤

## π”® κ·Έλ μ΄λ””μ–ΈνΈ λ¶€μ¤ν…

κΉμ΄κ°€ μ–•μ€ κ²°μ • νΈλ¦¬λ¥Ό μ‚¬μ©ν•μ—¬ μ΄μ „ νΈλ¦¬μ μ¤μ°¨λ¥Ό λ³΄μ™„ν•λ” λ°©μ‹μΌλ΅ μ•™μƒλΈ” ν•λ” λ°©λ²•μ΄λ‹¤. -> β“ μ•™μƒλΈ”ν•λ‹¤λ”κ² λ­”λ° γ…‹γ…‹γ…‹γ…‹ λ„λ€μ²΄,,λ­”κ°€ μƒν”λ“¤μ„ μ„μ–΄μ„ κ²°κ³Όλ¥Ό μμΈ΅ν•κ³  κ·Έ κ²°κ³Όμ ν‰κ· μ΄λ“  ν™•λ¥ μ΄λ“  μ„ κµ¬ν•λ” κ±΄κ°€?

### π“ ν•™μµ λ°©λ²•

GradientBoosingClassifier λ” κΈ°λ³Έμ μΌλ΅ κΉμ΄κ°€ 3μΈ κ²°μ • νΈλ¦¬λ¥Ό 100κ° μ‚¬μ©ν•λ‹¤. 4μ¥μ—μ„ λ°°μ› λ κ²½μ‚¬ ν•κ°•λ²•μ„ μ‚¬μ©ν•μ—¬ νΈλ¦¬λ¥Ό μ•™μƒλΈ”μ— μ¶”κ°€ν•λ”λ° λ¶„λ¥μ—μ„λ” λ΅μ§€μ¤ν‹± μ†μ‹¤ ν•¨μλ¥Ό μ‚¬μ©ν•κ³ , νκ·€μ—μ„λ” ν‰κ·  μ κ³± μ¤μ°¨ ν•¨μλ¥Ό μ‚¬μ©ν•λ‹¤.

### π“ ν›λ ¨ μ„ΈνΈ μ μ, κ²€μ¦ μ„ΈνΈ μ μ κµ¬ν•κΈ°

```python
from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.8881086892152563 # ν›λ ¨ μ„ΈνΈ μ μ
0.8720430147331015 # κ²€μ¦ μ„ΈνΈ μ μ
```

κ²°μ • νΈλ¦¬μ κ°μλ” κΈ°λ³Έμ μΌλ΅ 100κ°μ΄λ‹¤. μ΄λ¥Ό n_estimatorsλ¥Ό ν†µν•΄μ„ 500κ°λ΅ λλ ¤λ„ κ³Όλ€ μ ν•©μ„ μ μ–µμ ν•κ³  μλ‹¤.

```python
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.9464595437171814
0.8780082549788999
```

### π“ νΉμ„± μ¤‘μ”λ„

```python
gb.fit(train_input, train_target)
print(gb.feature_importances_)

>>>
[0.15872278 0.68010884 0.16116839]
```

## π”® νμ¤ν† κ·Έλ¨ κΈ°λ° κ·Έλ μ΄λ””μ–ΈνΈ λ¶€μ¤ν…

κ·Έλƒ¥ κ·Έλ μ΄λνΈ λ¶€μ¤ν…μ κ²½μ° νΈλ¦¬λ¥Ό μμ„λ€λ΅ μ¶”κ°€ν•λ©΄μ„ ν›λ ¨ν•λ” κ²ƒμ΄κΈ° λ•λ¬Έμ—, ν›λ ¨ μ†λ„κ°€ λλ¦¬λ‹¤. μ—¬κΈ°μ„ μ†λ„μ™€ μ„±λ¥μ„ λ”μ± κ°μ„ ν• κ±°μ‹± νμ¤ν† κ·Έλ¨ κΈ°λ° **_κ·Έλ μ΄λ””μ–ΈνΈ λ¶€μ¤ν…_** μ΄λ‹¤. μ •ν• λ°μ΄ν„°λ¥Ό λ‹¤λ£¨λ” λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦ μ¤‘ κ°€μ¥ μΈκΈ°κ°€ λ†’μ€ μ•κ³ λ¦¬μ¦μ΄λ‹¤. <br>

μ •ν• λ°μ΄ν„°λ¥Ό λ‹¤λ£¨λ” λ¨Έμ‹  λ¬λ‹ μ•κ³ λ¦¬μ¦ μ¤‘μ— κ°€μ¥ μΈκΈ°κ°€ λ†’μ€ μ•κ³ λ¦¬μ¦μ΄λ‹¤. μ…λ ¥ νΉμ„±μ„ 256κ°μ κµ¬κ°„μΌλ΅ λ‚λ„κ³  λ…Έλ“λ¥Ό λ¶„ν• ν•  λ• μµμ μ λ¶„ν• μ„ λ§¤μ° λΉ λ¥΄κ² μ°Ύμ„ μ μλ‹¤.

### π“ ν›λ ¨ μ„ΈνΈ μ μ, κ²€μ¦ μ„ΈνΈ μ μ κµ¬ν•κΈ°

```python
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))

>>>
0.9321723946453317
0.8801241948619236
```

### π“ νΉμ„± μ¤‘μ”λ„

νμ¤ν† κ·Έλ¨ κΈ°λ° κ·Έλ μ΄λ””μ–ΈνΈ λ¶€μ¤ν…μ νΉμ„± μ¤‘μ”λ„λ¥Ό κ³„μ‚°ν•κΈ° μ„ν•΄ permutation*importance() ν•¨μλ¥Ό μ‚¬μ©ν•κ² λ‹¤. (λ‹¤λ¥Έ νΈλ¦¬λ“¤μ€ κ°μ²΄.feature_importances*λ¥Ό ν†µν•΄μ„ μ¤‘μ”λ„ ν™•μΈ)

```python
from sklearn.inspection import permutation_importance

hgb.fit(train_input, train_target)
result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1)
print(result.importances_mean)

>>> [0.08876275 0.23438522 0.08027708]
```

- n_repeats : λλ¤ν•κ² μ„μ„ νμ μ§€μ •
  <br> <br>

- result.importances : νΉμ„± μ¤‘μ”λ„
- result.importances_mean : νΉμ„± μ¤‘μ”λ„ ν‰κ· 
- result.importances_std : νΉμ„± μ¤‘μ”λ„μ ν‘μ¤€ νΈμ°¨
  <br> <br>

νμ¤ν† κ·Έλ¨ κΈ°λ° κ·Έλ μ΄λ””μ–ΈνΈ λ¶€μ¤ν…μ νκ·€ λ²„μ „ : HistGradientBoostingRegressor ν΄λμ¤

### π“ μ‚¬μ΄ν‚·λ° μ™Έμ νμ¤ν† κ·Έλ¨ κΈ°λ° κ·Έλ μ΄λ””μ–ΈνΈ λ¶€μ¤ν… μ•κ³ λ¦¬μ¦μ„ κµ¬ν„ν• λΌμ΄λΈλ¬λ¦¬

- XGBoost

  ```python
  from xgboost import XGBClassifier
  xgb = XGBClassifier(tree_method='hist', random_state=42)
  scores = cross_validate(xgb, train_input, train_target, return_train_score=True)
  print(np.mean(scores['train_score']), np.mean(scores['test_score']))

  >>>
  0.8824322471423747
  0.8726214185237284
  ```

  μ‚¬μ΄ν‚·λ°μ cross_validate() ν•¨μμ™€ ν•¨κ» μ‚¬μ©ν•  μ μλ‹¤. tree_methodλ¥Ό 'his'λ΅ μ§€μ •ν•λ©΄ XGBoostκ°€ μ•„λ‹ νμ¤ν† κ·Έλ¨ κΈ°λ° κ·Έλ μ΄λ””μ–ΈνΈ λ¶€μ¤ν…μ„ μ‚¬μ©ν•  μ μλ‹¤.

- LightGBM

  ```python
  from lightgbm import LGBMClassifier
  lgb = LGBMClassifier(random_state=42)
  scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
  print(np.mean(scores['train_score']), np.mean(scores['test_score']))

  >>>
  0.9338079582727165
  0.8789710890649293
  ```
